{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes list of text fields (e.g. summary or reviewText fields) and\n",
    "# tokenizes, removes stop words and stems. Returns result as array of \n",
    "# lists, one list per review\n",
    "def preprocess_data(doc_set):    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        if not i:\n",
    "            i = ' '\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        tokens.append('null__') # add a bias term, will work as a kind of prior, important for empty reviews\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "# takes an array of lists as input, product labels, uniq_labels, and ratings,\n",
    "# and merges lists with matching labels among labels uniq_labels, averages\n",
    "# reviews belonging to the same, returns merged lists, and averaged ratings\n",
    "# uniq_labels should typically be np.unique(product labels), however \n",
    "# the option of specifying a subset is useful for parallelization to allow\n",
    "# different subsets to be processed by different engines\n",
    "def combine_reviews(text, asins):\n",
    "        products = [asins[0]]\n",
    "        combined_text = [text[0]]\n",
    "\n",
    "        #combine all the summaries into a single text and avg the review ratings for each product\n",
    "        for i in range(1, len(asins)):\n",
    "            last_element_index = len(products) - 1\n",
    "            if(asins[i] == products[last_element_index]):\n",
    "                combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "                \n",
    "            else:\n",
    "                products.append(asins[i])\n",
    "                combined_text.append(text[i])\n",
    "        \n",
    "        return (combined_text, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and prepare test data\n",
    "with open('../data/Sports_and_Outdoors_Reviews_test.json', 'r') as fp:\n",
    "    json_dat = [json.loads(x) for x in fp.readlines()]\n",
    "\n",
    "#json_dat = json_dat\n",
    "json_dat = sorted(json_dat, key=lambda k: k['asin'])\n",
    "    \n",
    "doc_list = []\n",
    "asin = []\n",
    "test_reviewer_id = []\n",
    "test_unixreviewtime = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    doc_list.append(json_dat[i].get('reviewText')\n",
    "    asin.append(json_dat[i].get('asin'))\n",
    "    test_reviewer_id.append(json_dat[i].get('reviewerID'))\n",
    "    test_unixreviewtime.append(json_dat[i].get('unixReviewTime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~96 CPU minutes\n",
    "\n",
    "# this cell runs things in parallel. make sure to start an \n",
    "# ipython cluster from the notebook dashboard's IPython Cluster\n",
    "# tab before running\n",
    "import ipyparallel as ipp\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "dview.execute('from nltk.tokenize import RegexpTokenizer;' +\n",
    "              'from nltk.corpus import stopwords; ' + \n",
    "              'from nltk.stem.porter import PorterStemmer;' +\n",
    "              'import numpy as np;')\n",
    "\n",
    "\n",
    "# clean text\n",
    "dview.push(dict(preprocess_data=preprocess_data))\n",
    "dview.scatter('doc_list', doc_list) # partitions data\n",
    "\n",
    "%px cleaned_reviews = preprocess_data(doc_list)\n",
    "cleaned_reviews = dview.gather('cleaned_reviews').get()\n",
    "\n",
    "# combine text\n",
    "total_text, uniq_prod_id = combine_reviews(cleaned_reviews, prod_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions for test data\n",
    "pred_lbls = clf.predict(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.column_stack((uniq_prod_id, pred_lbls.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Sports_and_Outdoors_Ratings_test.csv\", dat, delimiter=\",\", fmt=['%s', '%s'], \n",
    "           header='asin,awesomeReview')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of naive bayes classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ipyparallel as ipp\n",
    "\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import LsiModel as lsi\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import matutils\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Transformers for sklearn pipelines ###\n",
    "##########################################\n",
    "\n",
    "\n",
    "# this is a class to accommodate semantic space mappings. It takes\n",
    "# a bow representation as input and returns features in a latent\n",
    "# semantic space as output\n",
    "#\n",
    "# The class is a valid sklearn transformer and can be used as such\n",
    "# in sklearn pipelines. For details refer to,\n",
    "# https://scikit-learn.org/stable/modules/compose.html\n",
    "#\n",
    "# Also, this is another useful reference,\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "class docTopTransformer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, this_dict=None, d=300, distributed=False):\n",
    "        self.this_dict = this_dict\n",
    "        self.d = d\n",
    "        self.distributed = distributed\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        corpus = matutils.Dense2Corpus(np.transpose(X))\n",
    "        \n",
    "        # construct a semantic model based on document-topic similarity (15-20 min for 1500k reviews?)\n",
    "        self.semSpace = lsi(corpus, id2word=self.this_dict, num_topics=self.d, \n",
    "                            chunksize=20000, distributed=self.distributed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        corpus = matutils.Dense2Corpus(np.transpose(X))\n",
    "        \n",
    "        # Apply the semantic model to the training set bag of words (fast)\n",
    "        feat = self.semSpace[corpus]\n",
    "\n",
    "        # convert from TransformedCorpus datatype to numpy doc x topic array (medium speed, needs more benchmarking)\n",
    "        topics_csr = matutils.corpus2csc(feat)\n",
    "        X_ = topics_csr.T.toarray()\n",
    "        \n",
    "        return X_\n",
    "    \n",
    "\n",
    "# transforms documents to bag of word representations\n",
    "class doc2Bow(TransformerMixin, BaseEstimator):\n",
    "        \n",
    "    def _getBOW(self,X):\n",
    "        # transform corpus (train) into a 2d array word counts (a 'bag of words')\n",
    "        bow = [self.this_dict.doc2bow(text) for text in X]\n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    # takes corpus as input\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # train a document-topic model        \n",
    "        self.this_dict = Dictionary(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        bow = self._getBOW(X)\n",
    "        \n",
    "        X_ = np.transpose(matutils.corpus2dense(bow, len(self.this_dict)))\n",
    "        \n",
    "        return X_\n",
    "\n",
    "    \n",
    "# computes a profile for each subject specifying their likelihood of\n",
    "# submitting a particular rating. Transforms features (subject identifiers)\n",
    "# into a feature vector of log likelihoods representing the likelihood\n",
    "# of a particular rating given the identity of the subject providing the\n",
    "# rating\n",
    "class subjLogLikProfiles(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def _sumByLbl(self, X, lbls, laplaceSmooth=None):\n",
    "        # precomute any smoothing factors (SF)\n",
    "        if laplaceSmooth is not None:\n",
    "            numSF = np.multiply(laplaceSmooth['alpha']*laplaceSmooth['d'], laplaceSmooth['mu'])\n",
    "            denSF = laplaceSmooth['alpha']*laplaceSmooth['d']\n",
    "        else:\n",
    "            numSF = 0\n",
    "            denSF = 0\n",
    "\n",
    "        # sort data for efficient averaging\n",
    "        dat = sorted(list(zip(X,lbls)), key=lambda id: id[1])\n",
    "        dat = [[i for i,j in dat], [j for i,j in dat]]\n",
    "        X = np.array(dat[0])\n",
    "        lbls = dat[1]\n",
    "\n",
    "        uniq_lbls = np.unique(lbls)\n",
    "        uniq_lbls = sorted(uniq_lbls)\n",
    "\n",
    "        # use an averaging algorithm optimized for sorted entries\n",
    "        # (requires sorted search targets and search list)\n",
    "        # this algorithm never traverses the same element of the\n",
    "        # search list twice, but carries the overhead of a pre-\n",
    "        # sorted target list and search list. Thankfully those\n",
    "        # can use the O(n log(n)) python sort implementation\n",
    "        idx = 0\n",
    "        cum_X = np.zeros((len(uniq_lbls),len(X[0])))\n",
    "        \n",
    "        for i,this_id in enumerate(uniq_lbls):\n",
    "            idx = linearSearch(lbls, this_id, idx)\n",
    "            n = 0.0\n",
    "            while idx < len(lbls) and lbls[idx] == this_id:\n",
    "                cum_X[i] = np.sum(np.vstack([cum_X[i],X[idx]]),axis=0)\n",
    "                n += 1.0\n",
    "                idx += 1\n",
    "            cum_X[i] += numSF\n",
    "            cum_X[i] *= n/(n + denSF)\n",
    "\n",
    "        return cum_X, uniq_lbls\n",
    "    \n",
    "    # asumes X is tuple (prsn_id, prsn_rating, prod_id)\n",
    "    def __init__(self, laplaceSmooth=None):\n",
    "        self.laplaceSmooth = laplaceSmooth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        yLbl = np.unique(y)\n",
    "        \n",
    "        # mu is 'empirical' estimate it from empirical incidences\n",
    "        if self.laplaceSmooth:\n",
    "            if type(self.laplaceSmooth['mu']) is str and self.laplaceSmooth['mu'] == 'empirical':\n",
    "                mu = np.zeros(self.laplaceSmooth['d'])\n",
    "                for i in range(0, self.laplaceSmooth['d']):\n",
    "                    mu[i] = np.mean(y == yLbl[i])\n",
    "                \n",
    "                self.laplaceSmooth['mu'] = mu\n",
    "                \n",
    "            if type(self.laplaceSmooth['mu']) is str and self.laplaceSmooth['mu'] == 'uniform':\n",
    "                self.laplaceSmooth['mu'] = [1.0/self.laplaceSmooth['d']]*self.laplaceSmooth['d']\n",
    "        \n",
    "        # return frequencies of each review label for this subject\n",
    "        yExp = [y==yLbl[0]]\n",
    "        for i in range(1,len(yLbl)):\n",
    "            yExp = np.vstack([yExp, [y==yLbl[i]]])\n",
    "        yExp = np.transpose(yExp)\n",
    "                \n",
    "        subjectClassIncidence, uniq_prsn_id = self._sumByLbl(yExp, X, self.laplaceSmooth)\n",
    "        \n",
    "        classIncidence = np.sum(yExp,axis=0)\n",
    "        classIncidence = np.tile(classIncidence, len(subjectClassIncidence)\n",
    "                                ).reshape(len(subjectClassIncidence),len(classIncidence))\n",
    "        \n",
    "        logLik = np.subtract(np.log(subjectClassIncidence), np.log(classIncidence))\n",
    "        \n",
    "        self.profile = dict(zip(uniq_prsn_id, logLik))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        meanValue = np.mean(np.array(list(self.profile.values())), axis=0)\n",
    "        \n",
    "        profile = [meanValue]*len(X)\n",
    "        for i, rid in enumerate(X):\n",
    "            if rid in self.profile:\n",
    "                profile[i] = self.profile[rid]\n",
    "        \n",
    "        profile = np.vstack(profile)\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "# a function designed for loglikelihood normalization across reviews\n",
    "# converts likelihoods to posterior probabilities using a uniform prior\n",
    "class logLik2PostProb(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        marginal = np.log(np.sum(np.exp(X), axis=1))\n",
    "        n, m = np.shape(X)\n",
    "        marginal = np.transpose(np.tile(marginal, m).reshape(m,n))\n",
    "        \n",
    "        return np.exp(np.subtract(X, marginal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into (training, testing). \"testing\" in this context is a validation\n",
    "# dataset we use internally as a sanity check\n",
    "def split_data(path, percent):\n",
    "    import json\n",
    "    import math\n",
    "    \n",
    "    with open(path, 'r') as fp:\n",
    "        all_objs = [json.loads(x) for x in fp.readlines()]\n",
    "        \n",
    "    index = math.floor((percent/100)*len(all_objs))\n",
    "    training = []\n",
    "    test = []\n",
    "    for x in all_objs[:index]:\n",
    "        if x['asin'] not in all_objs[index]['asin']:\n",
    "            training.append(x)\n",
    "        else:\n",
    "            test.append(x)\n",
    "    \n",
    "    for x in all_objs[index:]:\n",
    "        test.append(x)\n",
    "        \n",
    "    return (training, test)\n",
    "\n",
    "# takes list of text fields (e.g. summary or reviewText fields) and\n",
    "# tokenizes, removes stop words and stems. Returns result as array of \n",
    "# lists, one list per review\n",
    "def preprocess_data(doc_set):    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        if not i:\n",
    "            i = ' '\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        tokens.append('null__') # add a bias term, will work as a kind of prior, important for empty reviews\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "# takes an array of lists as input, product labels, uniq_labels, and ratings,\n",
    "# and merges lists with matching labels among labels uniq_labels, averages\n",
    "# reviews belonging to the same, returns merged lists, and averaged ratings\n",
    "# uniq_labels should typically be np.unique(product labels), however \n",
    "# the option of specifying a subset is useful for parallelization to allow\n",
    "# different subsets to be processed by different engines\n",
    "def combine_reviews(text, asins, ratings):\n",
    "        products = [asins[0]]\n",
    "        combined_text = [text[0]]\n",
    "        average_rating = []\n",
    "        total_rating = ratings[0]\n",
    "        count = 1\n",
    "\n",
    "        #combine all the summaries into a single text and avg the review ratings for each product\n",
    "        for i in range(1, len(asins)):\n",
    "            last_element_index = len(products) - 1\n",
    "            if(asins[i] == products[last_element_index]):\n",
    "                combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "                total_rating += ratings[i]\n",
    "                count += 1\n",
    "            else:\n",
    "                average_rating.append(total_rating/count)\n",
    "                products.append(asins[i])\n",
    "                combined_text.append(text[i])\n",
    "                total_rating = ratings[i]\n",
    "                count = 1\n",
    "        average_rating.append(total_rating/count)\n",
    "        \n",
    "        return (combined_text, products, average_rating)\n",
    "    \n",
    "# similar to combine_review but removes rating averaging for test data. Test data has no review\n",
    "# field, so we can't average over one\n",
    "def combine_test_reviews(text, asins):\n",
    "        products = [asins[0]]\n",
    "        combined_text = [text[0]]\n",
    "\n",
    "        #combine all the summaries into a single text and avg the review ratings for each product\n",
    "        for i in range(1, len(asins)):\n",
    "            last_element_index = len(products) - 1\n",
    "            if(asins[i] == products[last_element_index]):\n",
    "                combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "                \n",
    "            else:\n",
    "                products.append(asins[i])\n",
    "                combined_text.append(text[i])\n",
    "        \n",
    "        return (combined_text, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for first match to target in dat, beginning\n",
    "# search at start_offset\n",
    "# useful for searching sorted lists.\n",
    "def linearSearch(dat, target, start_offset=0):\n",
    "    for i in range(start_offset, len(dat)):\n",
    "        if target == dat[i]:\n",
    "            return i\n",
    "\n",
    "# takes n x 1 vectors of prsn_ratings and matching prsn_id,\n",
    "# and an m x 1 (n >= m) vector of uniq_prsn_ids for whom we\n",
    "# want to get average X. Does not preserve order.\n",
    "# returns new uniq_lbls corresponding to order of avg_X\n",
    "# O( n log(n) )\n",
    "def avgByLbl(X, lbls):    \n",
    "    # sort data for efficient averaging\n",
    "    dat = sorted(list(zip(X,lbls)), key=lambda id: id[1])\n",
    "    dat = [[i for i,j in dat], [j for i,j in dat]]\n",
    "    X = np.array(dat[0])\n",
    "    lbls = dat[1]\n",
    "    \n",
    "    uniq_lbls = np.unique(lbls)\n",
    "    uniq_lbls = sorted(uniq_lbls)\n",
    "    \n",
    "    # use an averaging algorithm optimized for sorted entries\n",
    "    # (requires sorted search targets and search list)\n",
    "    # this algorithm never traverses the same element of the\n",
    "    # search list twice, but carries the overhead of a pre-\n",
    "    # sorted target list and search list. Thankfully those\n",
    "    # can use the O(n log(n)) python sort implementation\n",
    "    idx = 0\n",
    "    avg_X = np.zeros(len(uniq_lbls))\n",
    "    for i,this_id in enumerate(uniq_lbls):\n",
    "        idx = linearSearch(lbls, this_id, idx)\n",
    "        n = 0.0\n",
    "        while idx < len(lbls) and lbls[idx] == this_id:\n",
    "            avg_X[i] += X[idx]\n",
    "            n += 1.0\n",
    "            idx += 1\n",
    "        avg_X[i] /= n\n",
    "\n",
    "    return avg_X, uniq_lbls\n",
    "\n",
    "# computes expected rating based on probability of each class label\n",
    "def expRating(prob):\n",
    "    n,m = np.shape(prob)\n",
    "    values = [1,2,3,4,5]\n",
    "    values = np.tile(values,n).reshape(n,m)\n",
    "    exp_val = np.sum(np.multiply(prob, values), axis=1)\n",
    "    return exp_val\n",
    "\n",
    "def getAllRatings():   \n",
    "    prsn_asin = []\n",
    "    prsn_id = []\n",
    "    prsn_rating = []\n",
    "    with open('../data/Sports_and_Outdoors_Ratings_training.csv') as file:\n",
    "        reader = pd.read_csv(file, delimiter=',')\n",
    "        prsn_rating = np.array([item[1] for item in reader['overall'].items()])\n",
    "        prsn_id = np.array([item[1] for item in reader['reviewerID'].items()])\n",
    "        prsn_asin = np.array([item[1] for item in reader['asin'].items()])\n",
    "        \n",
    "    return prsn_rating, prsn_id, prsn_asin\n",
    "\n",
    "\n",
    "def getProdRatings(target_prod_id):\n",
    "    prsn_rating, prsn_id, prsn_asin = getAllRatings()\n",
    "    \n",
    "    prod_rating, prod_asin = avgByLbl(prsn_rating, prsn_asin)\n",
    "    \n",
    "    # sort prod_asin and target_prod_id so that they match\n",
    "    # save inverse sort function to reverse at the end\n",
    "    idx = np.argsort(target_prod_id)\n",
    "    inv_sort = np.argsort(idx)\n",
    "    \n",
    "    target_prod_id = np.array(target_prod_id)\n",
    "    target_prod_id = target_prod_id[idx]\n",
    "    prod_list = sorted(list(zip(prod_rating, prod_asin)), key=lambda id: id[1])\n",
    "    prod_rating = [i for i,j in prod_list]\n",
    "    prod_asin = [j for i,j in prod_list]\n",
    "    \n",
    "    # now we can assume that prod_ratings will match target_prod_id because both prod_asin and \n",
    "    # target_prod_id are sorted\n",
    "    prod_rating = [prod_rating[i] for i, this_prod in enumerate(prod_asin) if this_prod in target_prod_id] \n",
    "    prod_rating = np.array(prod_rating)\n",
    "    \n",
    "    # invert prod_rating to match original target_prod_rating order and return\n",
    "    return prod_rating[inv_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNRC(filename, stemmed=True):\n",
    "    \"\"\" Reads the NRC lexicon into a dictionary.\n",
    "    \"\"\"\n",
    "    wordToEmotions = dict()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    count = 0\n",
    "    with open(filename, 'r') as fp:\n",
    "        # Loop through lines\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            words = line.split('\\t')\n",
    "            if len(words) != 3:\n",
    "                continue\n",
    "            # Stem word\n",
    "            word = p_stemmer.stem(words[0]) if stemmed else words[0]\n",
    "            val = int(line[-1:])\n",
    "            # Store the emotions associated with the word\n",
    "            if count == 0:\n",
    "                wordToEmotions[word] = np.array([val])\n",
    "            else:\n",
    "                wordToEmotions[word] = np.append(wordToEmotions[word],val)\n",
    "                \n",
    "            count = (count + 1)%10\n",
    "    return wordToEmotions\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "def getEmotions(words, lexicon):\n",
    "    \"\"\" Returns a list with percentage of words which conveyed [anger, anticipation, ... , trust]\n",
    "    \"\"\"\n",
    "    emotionCount = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "    for word in words:\n",
    "        # Stem each word\n",
    "        word = p_stemmer.stem(word)\n",
    "        # Sum the emotions\n",
    "        if word in lexicon.keys():\n",
    "            emotionCount = emotionCount + lexicon[word]\n",
    "        \n",
    "    # Avg over all words\n",
    "    emotionCount = emotionCount / sum(emotionCount) if sum(emotionCount) > 0 else emotionCount\n",
    "    \n",
    "    return emotionCount\n",
    "\n",
    "def getDocEmotions(docList, lexicon):\n",
    "    \"\"\" docList is a list of list of words, lexicon is a dictionary of the NRC Word-Emotion Lexicon\n",
    "        Returns a list of emotion arrays.\n",
    "    \"\"\"\n",
    "    return [getEmotions(x, lexicon) for x in docList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_classifier(type, jobs=1):\n",
    "    if(type == 'LinearSVM'):\n",
    "        baseClf = LinearSVC()\n",
    "\n",
    "        params = {\n",
    "            'C': [1, 10, 100, 1000]\n",
    "        }\n",
    "\n",
    "        grid_LSVC = GridSearchCV(estimator = baseClf, param_grid = params, scoring = 'f1_macro', \n",
    "                       cv = 10, verbose = 1, n_jobs = 1)\n",
    "\n",
    "        n_estimators=10\n",
    "        bagClf = BaggingClassifier(base_estimator=grid_LSVC, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=jobs)\n",
    "        \n",
    "        baseClf = SVC() \n",
    "        doc2Top = docTopTransformer()\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', bagClf)]\n",
    "        clf = Pipeline(estimators)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'SVC'):\n",
    "        tuned_parameters = [{'clf__kernel': ['rbf'], \n",
    "                     'clf__gamma': [1e-1, 1e-2, 1e-3],\n",
    "                     'clf__C': [100, 1000, 10000]},\n",
    "                     {'clf__kernel': ['linear'], \n",
    "                      'clf__C': [100, 1000, 10000]},\n",
    "                     {'clf__kernel': ['poly'], \n",
    "                      'clf__C': [100, 1000, 10000],\n",
    "                      'clf__degree': [2]}]\n",
    "        \n",
    "        baseClf = SVC() \n",
    "        doc2Top = docTopTransformer()\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', baseClf)]\n",
    "        semClf = Pipeline(estimators)\n",
    "        \n",
    "        clf = GridSearchCV(semClf, tuned_parameters, cv=10, n_jobs=jobs, scoring='f1_macro')\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'BaggedDT'):\n",
    "        baseClf = DecisionTreeClassifier()\n",
    "\n",
    "        params = {\n",
    "            'clf__n_estimators': [5, 10, 100]\n",
    "        }\n",
    "\n",
    "        n_estimators=10\n",
    "        bagClf = BaggingClassifier(base_estimator=baseClf, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_jobs=1)\n",
    "\n",
    "        doc2Top = docTopTransformer()\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', bagClf)]\n",
    "        semClf = Pipeline(estimators)\n",
    "        \n",
    "        grid_LR = GridSearchCV(estimator = semClf, param_grid = params, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = jobs)\n",
    "        return grid_LR\n",
    "    \n",
    "    elif(type == 'RandomForest'):\n",
    "        baseClf = RandomForestClassifier()\n",
    "\n",
    "        \n",
    "        doc2Top = docTopTransformer()\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', baseClf)]\n",
    "        semClf = Pipeline(estimators)\n",
    "        \n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [10, 25, 50, 100]\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [5, 7, 10, 14]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "\n",
    "        #compiling all parameters into param_grid\n",
    "        param_grid = {\n",
    "            'clf__n_estimators': n_estimators, \n",
    "            'clf__max_depth': max_depth,\n",
    "            'clf__min_samples_split': min_samples_split,\n",
    "            'clf__min_samples_leaf': min_samples_leaf\n",
    "        }\n",
    "\n",
    "        #calling main classifier function\n",
    "        nestdClf = GridSearchCV(estimator = semClf, param_grid = param_grid, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = jobs)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'Boosted'):\n",
    "        boostedClf = AdaBoostClassifier()\n",
    "        \n",
    "        doc2Top = docTopTransformer()\n",
    "        # initialize a normalization transformer\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', boostedClf)]\n",
    "        clf = Pipeline(estimators)\n",
    "        \n",
    "        return clf\n",
    "        \n",
    "    elif(type == 'LogisticRegression'):\n",
    "        baseClf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "\n",
    "        params = {\n",
    "            'C': [10, 15, 20]\n",
    "        }\n",
    "\n",
    "        grid_LR = GridSearchCV(estimator = baseClf, param_grid = params, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = 1)\n",
    "\n",
    "        n_estimators=10\n",
    "        bagClf = BaggingClassifier(base_estimator=grid_LR, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=jobs)\n",
    "        \n",
    "        doc2Top = docTopTransformer()\n",
    "        # initialize a normalization transformer\n",
    "        norm_transformer = Normalizer()\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', bagClf)]\n",
    "        clf = Pipeline(estimators)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    else:\n",
    "        baseClf = GaussianNB()\n",
    "\n",
    "        n_estimators=10\n",
    "        bagClf = BaggingClassifier(base_estimator=baseClf, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=jobs)\n",
    "        \n",
    "        \n",
    "        doc2Top = docTopTransformer()\n",
    "        # initialize a normalization transformer\n",
    "        norm_transformer = Normalizer()\n",
    "\n",
    "        estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', bagClf)]\n",
    "        clf = Pipeline(estimators)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyByReviewerID():\n",
    "    prsn_rating, prsn_id, prsn_asin = getAllRatings()\n",
    "        \n",
    "    # GroupKFold gives you a KFold partitioner that abides by\n",
    "    # product labels so that products are only ever in a single\n",
    "    # fold\n",
    "    gkf = GroupKFold(n_splits=10)\n",
    "    cv = gkf.split(X, y, groups=lbl)\n",
    "\n",
    "    # alpha should be optimized\n",
    "    getLogLikProfiles = subjLogLikProfiles(laplaceSmooth=dict(alpha=0.2, d=5, mu='empirical'))\n",
    "    #getProfiles = subjProfile()\n",
    "\n",
    "    # NB isn't a great classifier but it's very fast\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    # get subject class likelihood profiles, convert to log likelihood, convert reviews\n",
    "    # to products (sum log likelihoods, average ratings and threshold for awesome),\n",
    "    # then classify ratings based on sum of log likelihoods (i.e. product of likelihoods)\n",
    "    # GaussianNB should automatically apply empirical class priors\n",
    "    reviewClf = Pipeline([('getLogLikProfiles', getLogLikProfiles), ('normalize',logLik2PostProb()), ('clf', nb)])\n",
    "    \n",
    "    X = prsn_id\n",
    "    y = prsn_rating\n",
    "    pred_review_scores = cross_val_predict(reviewClf, X, y, cv=cv, n_jobs=10)\n",
    "    \n",
    "\n",
    "    report = classification_report(y, pred_review_scores)\n",
    "    print('Subject Profile Prediction')\n",
    "    print(report)\n",
    "    \n",
    "    report = classification_report(y, np.random.permutation(pred_review_scores))\n",
    "    print('Null Prediction')\n",
    "    print(report)\n",
    "    \n",
    "    # plot predicted vs. observed ratings\n",
    "    from matplotlib import pyplot as plt\n",
    "    xx = y\n",
    "    yy = pred_review_scores\n",
    "    plt.rcParams.update({'font.size':18})\n",
    "    plt.figure(figsize=[6,6]);\n",
    "    plt.plot(xx, yy, '.');\n",
    "    plt.xlabel('Obs');\n",
    "    plt.ylabel('Pred');\n",
    "    plt.xlim([0,6]);\n",
    "    plt.ylim([0,6]);\n",
    "    plt.plot(np.unique(xx), np.poly1d(np.polyfit(xx, yy, 1))(np.unique(xx)));\n",
    "    plt.title('Review rating\\npredited by reviewer likelihood profile\\n');\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # above we predicted individual review ratings, here we're going to predict\n",
    "    # the overall awesome vs. not awesome rating\n",
    "    gkf = GroupKFold(n_splits=10)\n",
    "    cv = gkf.split(X, y, groups=lbl)\n",
    "\n",
    "    getLogLikProfiles = subjLogLikProfiles(laplaceSmooth=dict(alpha=0.2, d=5, mu='empirical'))\n",
    "    nb = GaussianNB()\n",
    "    calClf = CalibratedClassifierCV(nb)\n",
    "    reviewClf = Pipeline([('getLogLikProfiles', getLogLikProfiles), ('normalize',logLik2PostProb()), ('clf', calClf)])\n",
    "\n",
    "    prob_review_rating = cross_val_predict(reviewClf, X, y, cv=cv, method='predict_proba', n_jobs=10)\n",
    "    pred_review_rating = expRating(prob_review_rating)\n",
    "    pred_awesome = pred_review_rating > 4.5\n",
    "    \n",
    "    obs_awesome = y > 4.5\n",
    "        \n",
    "    report = classification_report(obs_awesome, pred_awesome)\n",
    "    print('Awesome Review Predicted by Subject Profile')\n",
    "    print(report)\n",
    "    report = classification_report(obs_awesome, np.random.permutation(pred_awesome))\n",
    "    print('Null Prediction')\n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    review_fpr, review_tpr, _ = roc_curve(obs_awesome, pred_review_rating)\n",
    "    review_auc = auc(review_fpr, review_tpr)\n",
    "\n",
    "    xx = review_fpr\n",
    "    yy = review_tpr\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plt.plot(xx, yy, label='ROC curve (AUC = %0.2f)' % review_auc, color='red')\n",
    "    plt.plot([0,1],[0,1],color='gray')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('Review predicted as awesome');\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # above we predicted reivews, now let's predict products \n",
    "    prob_prod_rating, new_lbl = avgByLbl(prob_review_rating, lbl)\n",
    "    pred_prod_rating = expRating(prob_prod_rating)\n",
    "    pred_prod_awesome = pred_prod_rating > 4.5\n",
    "\n",
    "    obs_rating, new_lbl = avgByLbl(y, lbl)\n",
    "    obs_awesome = obs_rating > 4.5\n",
    "    \n",
    "    \n",
    "    report = classification_report(obs_awesome, pred_prod_awesome)\n",
    "    print('Awesome Product Prediction by Subject Profile')\n",
    "    print(report)\n",
    "    report = classification_report(obs_awesome, np.random.permutation(pred_prod_awesome))\n",
    "    print('Null Prediction')\n",
    "    print(report)\n",
    "    \n",
    "    prod_fpr, prod_tpr, _ = roc_curve(obs_awesome, pred_prod_rating)\n",
    "    prod_auc = auc(prod_fpr, prod_tpr)\n",
    "\n",
    "    xx = prod_fpr\n",
    "    yy = prod_tpr\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plt.plot(xx, yy, label='ROC curve (AUC = %0.2f)' % prod_auc, color='red')\n",
    "    plt.plot([0,1],[0,1],color='gray')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('Review product as awesome');\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessJson(json_dat, test_dat=False):\n",
    "    #sort test data by asin\n",
    "    json_dat = sorted(json_dat, key=lambda k: k['asin'])\n",
    "    \n",
    "    summary = []\n",
    "    prod_id = []\n",
    "    rating = np.zeros(len(json_dat))\n",
    "    for i in range(0,len(json_dat)):\n",
    "        summary.append(json_dat[i].get('summary'))\n",
    "        if not test_dat:\n",
    "            rating[i] = json_dat[i].get('overall')\n",
    "        prod_id.append(json_dat[i].get('asin'))\n",
    "    \n",
    "    # this cell runs things in parallel. make sure to start an \n",
    "    # ipython cluster from the notebook dashboard's IPython Cluster\n",
    "    # tab before running\n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.execute('from nltk.tokenize import RegexpTokenizer;' +\n",
    "                  'from nltk.corpus import stopwords; ' + \n",
    "                  'from nltk.stem.porter import PorterStemmer;' +\n",
    "                  'import numpy as np;')\n",
    "\n",
    "    # clean text\n",
    "    dview.push(dict(preprocess_data=preprocess_data))\n",
    "    dview.scatter('summary', summary) # partitions data\n",
    "\n",
    "    %px cleaned_reviews = preprocess_data(summary)\n",
    "    cleaned_reviews = dview.gather('cleaned_reviews').get()\n",
    "\n",
    "    # combine text\n",
    "    total_text, uniq_prod_id, avg_ratings = combine_reviews(cleaned_reviews, prod_id, rating)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # vectorize training data\n",
    "    train_lbls = np.array(avg_ratings) >= 4.5\n",
    "    train_text = total_text\n",
    "    \n",
    "    return train_text, train_lbls, uniq_prod_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    ####### data input and preprocessing ############\n",
    "    \n",
    "    # Read in testing data for 80/20 split\n",
    "    # we won't use val_dat at all\n",
    "    json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "\n",
    "    train_text, train_lbls, uniq_prod_id = preprocessJson(json_dat)\n",
    "    \n",
    "    ######## estimate classifier accuracy #############\n",
    "\n",
    "    # initialize a transformer mapping from bow to latent semantic features\n",
    "    # pick a classifier\n",
    "    clf = pick_classifier('LogisticRegression')\n",
    "\n",
    "    # create a pipeline that transforms data to a bag of words \n",
    "    # representation, and passes that bag off to a classifier \n",
    "    # that may do any of a number of things to those features.\n",
    "    # most perform a dimensionality reduction using a document\n",
    "    # topic model before applying some cannonical algorithm \n",
    "    # in the document topic model space\n",
    "    estimators = [('doc2Bow', doc2Bow()), ('clf', clf)]\n",
    "    semClf = Pipeline(estimators)\n",
    "\n",
    "    # cross validate over the pipeline using 10-fold CV\n",
    "    pred_lbls = cross_val_predict(semClf, train_text, train_lbls, cv=10, n_jobs=4)\n",
    "    \n",
    "    true_lbls = getProdRatings(uniq_prod_id) > 4.5\n",
    "\n",
    "    # get classifier performance estimates\n",
    "    report = classification_report(true_lbls, pred_lbls)\n",
    "    print('Classifier performance')\n",
    "    print(report)\n",
    "    \n",
    "    # get null performance estimates\n",
    "    report = classification_report(true_lbls, np.random.permutation(pred_lbls))\n",
    "    print('Null performance')\n",
    "    print(report)\n",
    "    \n",
    "    ############# prepare final model ##############\n",
    "    \n",
    "    semClf.fit(train_text,train_lbls)\n",
    "\n",
    "    ############# make predictions on validation data ################\n",
    "    \n",
    "    val_text, val_lbls, val_prod_id = preprocessJson(val_dat)\n",
    "    \n",
    "    true_val_lbls = getProdRatings(val_prod_id) > 4.5\n",
    "        \n",
    "    val_pred = semClf.predict(val_text)\n",
    "        \n",
    "    # get classifier performance estimates\n",
    "    report = classification_report(true_val_lbls, val_pred)\n",
    "    print('Classifier performance')\n",
    "    print(report)\n",
    "    \n",
    "    # get null performance estimates\n",
    "    report = classification_report(true_val_lbls, np.random.permutation(val_pred))\n",
    "    print('Null performance')\n",
    "    print(report)\n",
    "    \n",
    "    ############# make predictions in test data ######################\n",
    "    \n",
    "    \n",
    "    # import and prepare test data\n",
    "    with open('../data/Sports_and_Outdoors_Reviews_test.json', 'r') as fp:\n",
    "        test_dat = [json.loads(x) for x in fp.readlines()]\n",
    "\n",
    "    test_text, _, test_prod_id = preprocessJson(test_dat)\n",
    "                        \n",
    "    # get model predictions for test data\n",
    "    test_lbls = semClf.predict(test_text)\n",
    "                        \n",
    "    #create output csv file from predictions\n",
    "    dat = np.column_stack((uniq_prod_id, pred_lbls.astype(int)))\n",
    "    np.savetxt(\"Sports_and_Outdoors_Ratings_test.csv\", dat, delimiter=\",\", fmt=['%s', '%s'], \n",
    "           header='asin,awesomeReview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of naive bayes classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ipyparallel as ipp\n",
    "\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import LsiModel as lsi\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import matutils\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a class to accommodate semantic space mappings. It takes\n",
    "# a bow representation as input and returns features in a latent\n",
    "# semantic space as output\n",
    "#\n",
    "# The class is a valid sklearn transformer and can be used as such\n",
    "# in sklearn pipelines. For details refer to,\n",
    "# https://scikit-learn.org/stable/modules/compose.html\n",
    "#\n",
    "# Also, this is another useful reference,\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "class docTopTransformer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, this_dict=None, d=300, distributed=False):\n",
    "        self.this_dict = this_dict\n",
    "        self.d = d\n",
    "        self.distributed = distributed\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        corpus = matutils.Dense2Corpus(np.transpose(X))\n",
    "        \n",
    "        # construct a semantic model based on document-topic similarity (15-20 min for 1500k reviews?)\n",
    "        self.semSpace = lsi(corpus, id2word=self.this_dict, num_topics=self.d, \n",
    "                            chunksize=20000, distributed=self.distributed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        corpus = matutils.Dense2Corpus(np.transpose(X))\n",
    "        \n",
    "        # Apply the semantic model to the training set bag of words (fast)\n",
    "        feat = self.semSpace[corpus]\n",
    "\n",
    "        # convert from TransformedCorpus datatype to numpy doc x topic array (medium speed, needs more benchmarking)\n",
    "        topics_csr = matutils.corpus2csc(feat)\n",
    "        X_ = topics_csr.T.toarray()\n",
    "        \n",
    "        return X_\n",
    "    \n",
    "\n",
    "# similar to docTopTransformer except it takes a corpus as input and trains dictionaries and computes BOWs internally\n",
    "class doc2Bow(TransformerMixin, BaseEstimator):\n",
    "        \n",
    "    def _getBOW(self,X):\n",
    "        # transform corpus (train) into a 2d array word counts (a 'bag of words')\n",
    "        bow = [self.this_dict.doc2bow(text) for text in X]\n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    # takes corpus as input\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # train a document-topic model        \n",
    "        self.this_dict = Dictionary(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        bow = self._getBOW(X)\n",
    "        \n",
    "        X_ = np.transpose(matutils.corpus2dense(bow, len(self.this_dict)))\n",
    "        \n",
    "        return X_\n",
    "    \n",
    "# similar to docTopTransformer except it takes a corpus as input and trains dictionaries and computes BOWs internally\n",
    "class docTopTransformer2(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, d=300, distributed=False):\n",
    "        self.this_dict = []\n",
    "        self.d = d\n",
    "        self.distributed = distributed\n",
    "        \n",
    "    def _getBOW(self,X):\n",
    "        # transform corpus (train) into a 2d array word counts (a 'bag of words')\n",
    "        bow = [self.this_dict.doc2bow(text) for text in X]\n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    # takes corpus as input\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # train a document-topic model        \n",
    "        self.this_dict = Dictionary(X)\n",
    "\n",
    "        bow = self._getBOW(X)\n",
    "        \n",
    "        # construct a semantic model based on document-topic similarity (15-20 min for 1500k reviews?)\n",
    "        self.semSpace = lsi(bow, id2word=self.this_dict, num_topics=self.d, \n",
    "                            chunksize=100000, distributed=self.distributed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        bow = self._getBOW(X)\n",
    "        \n",
    "        # Apply the semantic model to the training set bag of words (fast)\n",
    "        feat = self.semSpace[bow]\n",
    "\n",
    "        # convert from TransformedCorpus datatype to numpy doc x topic array (medium speed, needs more benchmarking)\n",
    "        topics_csr = matutils.corpus2csc(feat)\n",
    "        X_ = topics_csr.T.toarray()\n",
    "        \n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into (training, testing)\n",
    "def split_data(path, percent):\n",
    "    import json\n",
    "    import math\n",
    "    \n",
    "    with open(path, 'r') as fp:\n",
    "        all_objs = [json.loads(x) for x in fp.readlines()]\n",
    "        \n",
    "    index = math.floor((percent/100)*len(all_objs))\n",
    "    training = []\n",
    "    test = []\n",
    "    for x in all_objs[:index]:\n",
    "        if x['asin'] not in all_objs[index]['asin']:\n",
    "            training.append(x)\n",
    "        else:\n",
    "            test.append(x)\n",
    "    \n",
    "    for x in all_objs[index:]:\n",
    "        test.append(x)\n",
    "        \n",
    "    return (training, test)\n",
    "\n",
    "# takes list of text fields (e.g. summary or reviewText fields) and\n",
    "# tokenizes, removes stop words and stems. Returns result as array of \n",
    "# lists, one list per review\n",
    "def preprocess_data(doc_set):    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        if not i:\n",
    "            i = ' '\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        tokens.append('null__') # add a bias term, will work as a kind of prior, important for empty reviews\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "# takes an array of lists as input, product labels, uniq_labels, and ratings,\n",
    "# and merges lists with matching labels among labels uniq_labels, averages\n",
    "# reviews belonging to the same, returns merged lists, and averaged ratings\n",
    "# uniq_labels should typically be np.unique(product labels), however \n",
    "# the option of specifying a subset is useful for parallelization to allow\n",
    "# different subsets to be processed by different engines\n",
    "def combine_reviews(text, asins, ratings):\n",
    "        products = [asins[0]]\n",
    "        combined_text = [text[0]]\n",
    "        average_rating = []\n",
    "        total_rating = ratings[0]\n",
    "        count = 1\n",
    "\n",
    "        #combine all the summaries into a single text and avg the review ratings for each product\n",
    "        for i in range(1, len(asins)):\n",
    "            last_element_index = len(products) - 1\n",
    "            if(asins[i] == products[last_element_index]):\n",
    "                combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "                total_rating += ratings[i]\n",
    "                count += 1\n",
    "            else:\n",
    "                average_rating.append(total_rating/count)\n",
    "                products.append(asins[i])\n",
    "                combined_text.append(text[i])\n",
    "                total_rating = ratings[i]\n",
    "                count = 1\n",
    "        average_rating.append(total_rating/count)\n",
    "        \n",
    "        return (combined_text, products, average_rating)\n",
    "    \n",
    "#similar to combine_review but removes rating averaging for test data\n",
    "def combine_test_reviews(text, asins):\n",
    "        products = [asins[0]]\n",
    "        combined_text = [text[0]]\n",
    "\n",
    "        #combine all the summaries into a single text and avg the review ratings for each product\n",
    "        for i in range(1, len(asins)):\n",
    "            last_element_index = len(products) - 1\n",
    "            if(asins[i] == products[last_element_index]):\n",
    "                combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "                \n",
    "            else:\n",
    "                products.append(asins[i])\n",
    "                combined_text.append(text[i])\n",
    "        \n",
    "        return (combined_text, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches for first match to target in dat, beginning\n",
    "# search at start_offset\n",
    "# useful for searching sorted lists.\n",
    "def linearSearch(dat, target, start_offset=0):\n",
    "    for i in range(start_offset, len(dat)):\n",
    "        if target == dat[i]:\n",
    "            return i\n",
    "\n",
    "# takes n x 1 vectors of prsn_ratings and matching prsn_id,\n",
    "# and an m x 1 (n >= m) vector of uniq_prsn_ids for whom we\n",
    "# want to get average X. Does not preserve order.\n",
    "# returns new uniq_lbls corresponding to order of avg_X\n",
    "# O( n log(n) )\n",
    "#\n",
    "# if laplaceSmoothing is used then we add alpha to numerator\n",
    "# and alpha*d to the denominator. If empirical distribution\n",
    "# is known you can specify empirical distribution of category\n",
    "# using mu. Mu can be a vector of length m if X is n x m\n",
    "def avgByLbl(X, lbls):    \n",
    "    # sort data for efficient averaging\n",
    "    dat = sorted(list(zip(X,lbls)), key=lambda id: id[1])\n",
    "    dat = [[i for i,j in dat], [j for i,j in dat]]\n",
    "    X = np.array(dat[0])\n",
    "    lbls = dat[1]\n",
    "    \n",
    "    uniq_lbls = np.unique(lbls)\n",
    "    uniq_lbls = sorted(uniq_lbls)\n",
    "    \n",
    "    # use an averaging algorithm optimized for sorted entries\n",
    "    # (requires sorted search targets and search list)\n",
    "    # this algorithm never traverses the same element of the\n",
    "    # search list twice, but carries the overhead of a pre-\n",
    "    # sorted target list and search list. Thankfully those\n",
    "    # can use the O(n log(n)) python sort implementation\n",
    "    idx = 0\n",
    "    avg_X = np.zeros(len(uniq_lbls))\n",
    "    for i,this_id in enumerate(uniq_lbls):\n",
    "        idx = linearSearch(lbls, this_id, idx)\n",
    "        n = 0.0\n",
    "        while idx < len(lbls) and lbls[idx] == this_id:\n",
    "            avg_X[i] += X[idx]\n",
    "            n += 1.0\n",
    "            idx += 1\n",
    "        avg_X[i] /= n\n",
    "\n",
    "    return avg_X, uniq_lbls\n",
    "\n",
    "def getProdRatings(target_prod_id):\n",
    "    import pandas as pd\n",
    "\n",
    "    prsn_asin = []\n",
    "    prsn_id = []\n",
    "    prsn_rating = []\n",
    "    with open('../data/Sports_and_Outdoors_Ratings_training.csv') as file:\n",
    "        reader = pd.read_csv(file, delimiter=',')\n",
    "        prsn_rating = np.array([item[1] for item in reader['overall'].items()])\n",
    "        prsn_id = np.array([item[1] for item in reader['reviewerID'].items()])\n",
    "        prsn_asin = np.array([item[1] for item in reader['asin'].items()])\n",
    "\n",
    "    \n",
    "    prod_rating, prod_asin = avgByLbl(prsn_rating, prsn_asin)\n",
    "    \n",
    "    # sort prod_asin and target_prod_id so that they match\n",
    "    # save inverse sort function to reverse at the end\n",
    "    idx = np.argsort(target_prod_id)\n",
    "    inv_sort = np.argsort(idx)\n",
    "    \n",
    "    target_prod_id = np.array(target_prod_id)\n",
    "    target_prod_id = target_prod_id[idx]\n",
    "    prod_list = sorted(list(zip(prod_rating, prod_asin)), key=lambda id: id[1])\n",
    "    prod_rating = [i for i,j in prod_list]\n",
    "    prod_asin = [j for i,j in prod_list]\n",
    "    \n",
    "    # now we can assume that prod_ratings will match target_prod_id because both prod_asin and \n",
    "    # target_prod_id are sorted\n",
    "    prod_rating = [prod_rating[i] for i, this_prod in enumerate(prod_asin) if this_prod in target_prod_id] \n",
    "    prod_rating = np.array(prod_rating)\n",
    "    \n",
    "    # invert prod_rating to match original target_prod_rating order and return\n",
    "    return prod_rating[inv_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNRC(filename, stemmed=True):\n",
    "    \"\"\" Reads the NRC lexicon into a dictionary.\n",
    "    \"\"\"\n",
    "    wordToEmotions = dict()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    count = 0\n",
    "    with open(filename, 'r') as fp:\n",
    "        # Loop through lines\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            words = line.split('\\t')\n",
    "            if len(words) != 3:\n",
    "                continue\n",
    "            # Stem word\n",
    "            word = p_stemmer.stem(words[0]) if stemmed else words[0]\n",
    "            val = int(line[-1:])\n",
    "            # Store the emotions associated with the word\n",
    "            if count == 0:\n",
    "                wordToEmotions[word] = np.array([val])\n",
    "            else:\n",
    "                wordToEmotions[word] = np.append(wordToEmotions[word],val)\n",
    "                \n",
    "            count = (count + 1)%10\n",
    "    return wordToEmotions\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "def getEmotions(words, lexicon):\n",
    "    \"\"\" Returns a list with percentage of words which conveyed [anger, anticipation, ... , trust]\n",
    "    \"\"\"\n",
    "    emotionCount = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "    for word in words:\n",
    "        # Stem each word\n",
    "        word = p_stemmer.stem(word)\n",
    "        # Sum the emotions\n",
    "        if word in lexicon.keys():\n",
    "            emotionCount = emotionCount + lexicon[word]\n",
    "        \n",
    "    # Avg over all words\n",
    "    emotionCount = emotionCount / sum(emotionCount) if sum(emotionCount) > 0 else emotionCount\n",
    "    \n",
    "    return emotionCount\n",
    "\n",
    "def getDocEmotions(docList, lexicon):\n",
    "    \"\"\" docList is a list of list of words, lexicon is a dictionary of the NRC Word-Emotion Lexicon\n",
    "        Returns a list of emotion arrays.\n",
    "    \"\"\"\n",
    "    return [getEmotions(x, lexicon) for x in docList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "\n",
    "#sort test data by asin\n",
    "json_dat = sorted(json_dat, key=lambda k: k['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "review = []\n",
    "summary = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    rating.append(json_dat[i].get('overall'))\n",
    "    summary.append(json_dat[i].get('summary'))\n",
    "    review.append(json_dat[i].get('reviewText'))\n",
    "    prod_id.append(json_dat[i].get('asin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.22944235801697\n"
     ]
    }
   ],
   "source": [
    "# takes ~96 CPU minutes\n",
    "\n",
    "# this cell runs things in parallel. make sure to start an \n",
    "# ipython cluster from the notebook dashboard's IPython Cluster\n",
    "# tab before running\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "dview.execute('from nltk.tokenize import RegexpTokenizer;' +\n",
    "              'from nltk.corpus import stopwords; ' + \n",
    "              'from nltk.stem.porter import PorterStemmer;' +\n",
    "              'import numpy as np;')\n",
    "\n",
    "# clean text\n",
    "dview.push(dict(preprocess_data=preprocess_data))\n",
    "dview.scatter('summary', summary) # partitions data\n",
    "\n",
    "%px cleaned_reviews = preprocess_data(summary)\n",
    "cleaned_reviews = dview.gather('cleaned_reviews').get()\n",
    "\n",
    "# combine text\n",
    "total_text, uniq_prod_id, avg_ratings = combine_reviews(cleaned_reviews, prod_id, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del summary\n",
    "del review\n",
    "del json_dat\n",
    "del val_dat\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize training data\n",
    "train_lbls = np.array(avg_ratings) >= 4.5\n",
    "train_text = total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_classifier(type):\n",
    "    if(type == 'LinearSVM'):\n",
    "        baseClf = LinearSVC()\n",
    "\n",
    "        params = {\n",
    "            'C': [1, 10, 100, 1000]\n",
    "        }\n",
    "\n",
    "        grid_LSVC = GridSearchCV(estimator = baseClf, param_grid = params, scoring = 'f1_macro', \n",
    "                       cv = 10, verbose = 1, n_jobs = 1)\n",
    "\n",
    "        n_estimators=10\n",
    "        clf = BaggingClassifier(base_estimator=grid_LSVC, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=1)\n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'SVC'):\n",
    "        tuned_parameters = [{'clf__kernel': ['rbf'], \n",
    "                     'clf__gamma': [1e-1, 1e-2, 1e-3],\n",
    "                     'clf__C': [100, 1000, 10000]},\n",
    "                     {'clf__kernel': ['linear'], \n",
    "                      'clf__C': [100, 1000, 10000]},\n",
    "                     {'clf__kernel': ['poly'], \n",
    "                      'clf__C': [100, 1000, 10000],\n",
    "                      'clf__degree': [2]}]\n",
    "        \n",
    "        baseClf = SVC()\n",
    "        clf = GridSearchCV(semClf, tuned_parameters, cv=10, n_jobs=1, scoring='f1_macro')\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'BaggedDT'):\n",
    "        baseClf = DecisionTreeClassifier()\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [5, 10, 100]\n",
    "        }\n",
    "\n",
    "        n_estimators=10\n",
    "        clf = BaggingClassifier(base_estimator=baseClf, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_jobs=1)\n",
    "\n",
    "        grid_LR = GridSearchCV(estimator = clf, param_grid = params, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = 1)\n",
    "        return grid_LR\n",
    "    \n",
    "    elif(type == 'RandomForest'):\n",
    "        baseClf = RandomForestClassifier()\n",
    "\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [10, 25, 50, 100]\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [5, 7, 10, 14]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "\n",
    "        #compiling all parameters into param_grid\n",
    "        param_grid = {\n",
    "            'n_estimators': n_estimators, \n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf\n",
    "        }\n",
    "\n",
    "        #calling main classifier function\n",
    "        clf = GridSearchCV(estimator = baseClf, param_grid = param_grid, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = 1)\n",
    "        return clf\n",
    "    \n",
    "    elif(type == 'Boosted'):\n",
    "        boostedClf = AdaBoostClassifier()\n",
    "        return boostedClf\n",
    "        \n",
    "    elif(type == 'LogisticRegression'):\n",
    "        baseClf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "\n",
    "        params = {\n",
    "            'C': [10, 15, 20]\n",
    "        }\n",
    "\n",
    "        grid_LR = GridSearchCV(estimator = baseClf, param_grid = params, scoring = 'f1_macro', \n",
    "                               cv = 10, verbose = 1, n_jobs = 1)\n",
    "\n",
    "        n_estimators=10\n",
    "        clf = BaggingClassifier(base_estimator=grid_LR, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=1)\n",
    "        return clf\n",
    "    \n",
    "    else:\n",
    "        baseClf = GaussianNB()\n",
    "\n",
    "        n_estimators=10\n",
    "        clf = BaggingClassifier(base_estimator=baseClf, \n",
    "                                bootstrap=False, max_samples = 1.0/n_estimators, n_estimators=n_estimators,\n",
    "                                n_jobs=1)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3484.0885138511658\n"
     ]
    }
   ],
   "source": [
    "# estimate classifier accuracy\n",
    "\n",
    "# GroupKFold gives you a KFold partitioner that abides by\n",
    "# product labels so that products are only ever in a single\n",
    "# fold\n",
    "kf = KFold(n_splits=10)\n",
    "cv = kf.split(train_text, train_lbls)\n",
    "\n",
    "# initialize a transformer mapping from bow to latent semantic features\n",
    "doc2Top = docTopTransformer2()\n",
    "\n",
    "# initialize a normalization transformer\n",
    "norm_transformer = Normalizer()\n",
    "\n",
    "# pick a classifier\n",
    "clf = pick_classifier('LogisticRegression')\n",
    "\n",
    "# create a pipeline that transforms data to semantic space, \n",
    "# and then classifies them by averaging over n_estimators of \n",
    "# type baseClf\n",
    "#\n",
    "# Note, you could bag over n semantic models too by creating\n",
    "# a pipeline using bow2Top and baseClf, and then passing that\n",
    "# in as th base_estimator argument of a BaggingClassifier\n",
    "# instance. If you think bagging classification of reviews will\n",
    "# lead to better classification performance for theoretical\n",
    "# reasons, this would be the way to go, however the purpose\n",
    "# of bagging here is for to avoid a bypass the slow SVM fitting\n",
    "# procedure\n",
    "estimators = [('projection', doc2Top), ('normalization', norm_transformer), ('clf', clf)]\n",
    "semBagClf = Pipeline(estimators)\n",
    "\n",
    "# cross validate over the pipeline using group k-fold CV\n",
    "pred_lbls = cross_val_predict(semBagClf, train_text, train_lbls, cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True recall is sensitivity, false recall is specificity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.76      0.76     34984\n",
      "        True       0.74      0.75      0.75     32354\n",
      "\n",
      "    accuracy                           0.75     67338\n",
      "   macro avg       0.75      0.75      0.75     67338\n",
      "weighted avg       0.75      0.75      0.75     67338\n",
      "\n",
      "Accuracy: 0.754\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(train_lbls, pred_lbls)\n",
    "print('True recall is sensitivity, false recall is specificity')\n",
    "print(report)\n",
    "\n",
    "# this is not exactly the same as the average of each CV folds accuracy, \n",
    "# but it's close and much faster to compute\n",
    "acc = 1-np.mean(pred_lbls != train_lbls)\n",
    "print(\"Accuracy: %0.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=20, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=1570096259, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=668900618, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=1201338831, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=97066811, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=1976390254, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=1615181950, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=1092422327, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=2118837176, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=664069747, solver='saga')\n",
      "LogisticRegression(C=10, l1_ratio=0.5, penalty='elasticnet',\n",
      "                   random_state=820768831, solver='saga')\n"
     ]
    }
   ],
   "source": [
    "#fit the training data\n",
    "semBagClf = semBagClf.fit(train_text,train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fitted classifier\n",
    "\n",
    "with open('logisticRegression.clf',mode='wb') as f:\n",
    "    pickle.dump(semBagClf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and prepare test data\n",
    "with open('../data/Sports_and_Outdoors_Reviews_test.json', 'r') as fp:\n",
    "    json_dat = [json.loads(x) for x in fp.readlines()]\n",
    "\n",
    "#json_dat = json_dat\n",
    "json_dat = sorted(json_dat, key=lambda k: k['asin'])\n",
    "    \n",
    "doc_list = []\n",
    "asin = []\n",
    "test_reviewer_id = []\n",
    "test_unixreviewtime = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    doc_list.append(json_dat[i].get('reviewText')\n",
    "    asin.append(json_dat[i].get('asin'))\n",
    "    test_reviewer_id.append(json_dat[i].get('reviewerID'))\n",
    "    test_unixreviewtime.append(json_dat[i].get('unixReviewTime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~96 CPU minutes\n",
    "\n",
    "# this cell runs things in parallel. make sure to start an \n",
    "# ipython cluster from the notebook dashboard's IPython Cluster\n",
    "# tab before running\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "dview.execute('from nltk.tokenize import RegexpTokenizer;' +\n",
    "              'from nltk.corpus import stopwords; ' + \n",
    "              'from nltk.stem.porter import PorterStemmer;' +\n",
    "              'import numpy as np;')\n",
    "\n",
    "\n",
    "# clean text\n",
    "dview.push(dict(preprocess_data=preprocess_data))\n",
    "dview.scatter('doc_list', doc_list) # partitions data\n",
    "\n",
    "%px cleaned_reviews = preprocess_data(doc_list)\n",
    "cleaned_reviews = dview.gather('cleaned_reviews').get()\n",
    "\n",
    "# combine text\n",
    "total_text, uniq_prod_id = combine_test_reviews(cleaned_reviews, asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions for test data\n",
    "pred_lbls = semBagClf.predict(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output csv file from predictions\n",
    "dat = np.column_stack((uniq_prod_id, pred_lbls.astype(int)))\n",
    "np.savetxt(\"Sports_and_Outdoors_Ratings_test.csv\", dat, delimiter=\",\", fmt=['%s', '%s'], \n",
    "           header='asin,awesomeReview')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.12"
=======
   "version": "3.6.5"
>>>>>>> eb186ca221b018d8fd52bcdeed1a45a45c8832b1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

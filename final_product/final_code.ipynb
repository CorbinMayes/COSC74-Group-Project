{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of linear SVM classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data\n",
    "from semanticClassifiers import docTopTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "json_dat = json_dat[0:20000] # use this for prototyping on smaller subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "doc_list = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    #if json_dat[i].get('reviewText'): #not all reviews have text\n",
    "    if json_dat[i].get('summary'): #not all reviews have summary text\n",
    "        rating.append(json_dat[i].get('overall'))\n",
    "        doc_list.append(json_dat[i].get('summary'))\n",
    "        #doc_list.append(json_dat[i].get('reviewText'))\n",
    "        prod_id.append(json_dat[i].get('asin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=preprocess_data(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import gensim.matutils as matutils\n",
    "\n",
    "# vectorize training data\n",
    "train_lbls = np.array(rating) >= 4.5\n",
    "train_text = clean_text\n",
    "\n",
    "# train a document-topic model        \n",
    "this_dict = Dictionary(train_text)\n",
    "\n",
    "# transform corpus (train) into a 2d array word counts (a 'bag of words')\n",
    "bow_train = [this_dict.doc2bow(text) for text in train_text]\n",
    "bow_train = np.transpose(matutils.corpus2dense(bow_train, len(this_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:   58.0s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  35 out of  40 | elapsed:  2.2min remaining:   19.0s\n",
      "[Parallel(n_jobs=4)]: Done  37 out of  40 | elapsed:  2.5min remaining:   11.8s\n",
      "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159.02924251556396\n"
     ]
    }
   ],
   "source": [
    "# estimate classifier accuracy\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# GroupKFold gives you a KFold partitioner that abides by\n",
    "# product labels so that products are only ever in a single\n",
    "# fold\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "cv = gkf.split(bow_train, train_lbls, groups=prod_id)\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "# initialize a transformer mapping from bow to latent semantic features\n",
    "bow2Top = docTopTransformer(this_dict=this_dict, d=300, distributed=False)\n",
    "\n",
    "params = {\n",
    "    'clf__base_estimator__max_depth': [5, 7, 10, 15],\n",
    "    'clf__n_estimators': [10],\n",
    "    'clf__max_samples': [0.1]\n",
    "}\n",
    "\n",
    "# pick a classifier\n",
    "baseClf = DecisionTreeClassifier()\n",
    "\n",
    "clf = BaggingClassifier(base_estimator=baseClf, bootstrap=False, n_jobs = 4)\n",
    "\n",
    "# create a pipeline that transforms data to semantic space, \n",
    "# and then classifies them using clf\n",
    "estimators = [('projection', bow2Top), ('clf', clf)]\n",
    "semClf = Pipeline(estimators)\n",
    "\n",
    "grid_DT = GridSearchCV(estimator = semClf, param_grid = params, scoring = ['f1', 'accuracy', 'precision', 'recall'], \n",
    "                       refit = 'f1', cv = cv, verbose = 40, n_jobs = 4)\n",
    "\n",
    "grid_DT.fit(bow_train, train_lbls)\n",
    "\n",
    "# cross validate over the pipeline using group k-fold CV\n",
    "#pred_lbls = cross_val_predict(semClf, bow_train, train_lbls, cv=cv)\n",
    "\n",
    "time1 = time.time()\n",
    "print(time1-time0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('projection',\n",
      "                 docTopTransformer(d=300, distributed=False,\n",
      "                                   this_dict=<gensim.corpora.dictionary.Dictionary object at 0x0000024F4EFF4EC8>)),\n",
      "                ('clf',\n",
      "                 BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                                         class_weight=None,\n",
      "                                                                         criterion='gini',\n",
      "                                                                         max_depth=5,\n",
      "                                                                         max_features=None,\n",
      "                                                                         max_leaf_nodes=None,\n",
      "                                                                         min_impurity_decrease=0.0,\n",
      "                                                                         min_impurity_split=None,\n",
      "                                                                         min_samples_leaf=1,\n",
      "                                                                         min_samples_split=2,\n",
      "                                                                         min_weight_fraction_leaf=0.0,\n",
      "                                                                         presort='deprecated',\n",
      "                                                                         random_state=None,\n",
      "                                                                         splitter='best'),\n",
      "                                   bootstrap=False, bootstrap_features=False,\n",
      "                                   max_features=1.0, max_samples=0.1,\n",
      "                                   n_estimators=10, n_jobs=4, oob_score=False,\n",
      "                                   random_state=None, verbose=0,\n",
      "                                   warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      "\n",
      "best score:\n",
      "0.8406002344844481\n",
      "\n",
      "\n",
      "best params:\n",
      "{'clf__base_estimator__max_depth': 5, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}\n",
      "\n",
      "\n",
      "best index:\n",
      "0\n",
      "\n",
      "\n",
      "scorer:\n",
      "{'f1': make_scorer(f1_score, average=binary), 'accuracy': make_scorer(accuracy_score), 'precision': make_scorer(precision_score, average=binary), 'recall': make_scorer(recall_score, average=binary)}\n",
      "\n",
      "\n",
      "refit time:\n",
      "10.064373970031738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#print(grid_DT.cv_results_)\n",
    "print(grid_DT.best_estimator_)\n",
    "print(\"\\n\")\n",
    "print(\"best score:\")\n",
    "print(grid_DT.best_score_)\n",
    "print(\"\\n\")\n",
    "print(\"best params:\")\n",
    "print(grid_DT.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"best index:\")\n",
    "print(grid_DT.best_index_)\n",
    "print(\"\\n\")\n",
    "print(\"scorer:\")\n",
    "print(grid_DT.scorer_)\n",
    "print(\"\\n\")\n",
    "print(\"refit time:\")\n",
    "print(grid_DT.refit_time_)\n",
    "\n",
    "\n",
    "#report = classification_report(train_lbls, pred_lbls)\n",
    "#print('True recall is sensitivity, false recall is specificity')\n",
    "#print(report)\n",
    "\n",
    "# this is not exactly the same as the average of each CV folds accuracy, \n",
    "# but it's close and much faster to compute\n",
    "#acc = 1-np.mean(pred_lbls != train_lbls)\n",
    "#print(\"Accuracy: %0.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-eb5be6c0429b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Create prediction array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_DT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Create output file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \"\"\"\n\u001b[0;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_estimator_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CS74\\COSC74-Group-Project\\libraries\\semanticClassifiers.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# convert from TransformedCorpus datatype to numpy doc x topic array (medium speed, needs more benchmarking)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mtopics_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mX_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics_csr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36mcorpus2csc\u001b[1;34m(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;31m# slower version; determine the sparse matrix parameters during iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mnum_nnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprintprogress\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdocno\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprintprogress\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PROGRESS: at document #%i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[1;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m             \u001b[0mwrapped_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped_chunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mfull2sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'flat'"
     ]
    }
   ],
   "source": [
    "#Prediction cell\n",
    "import json\n",
    "import math\n",
    "\n",
    "#read in the file\n",
    "path = '../data/Sports_and_Outdoors_Reviews_test.json'\n",
    "with open(path, 'r') as fp:\n",
    "    all_reviews = [json.loads(x) for x in fp.readlines()]\n",
    "\n",
    "# create a list, doc_list, with one review per elem\n",
    "test_doc_list = []\n",
    "test_prod_id = []\n",
    "test_reviewer_id = []\n",
    "test_unixreviewtime = []\n",
    "for i in range(0,len(all_reviews)):\n",
    "    #if json_dat[i].get('reviewText'): #not all reviews have text\n",
    "    if all_reviews[i].get('summary'): #not all reviews have summary text\n",
    "        test_doc_list.append(all_reviews[i].get('summary'))\n",
    "        #test_doc_list.append(all_reviews[i].get('reviewText'))\n",
    "        test_prod_id.append(all_reviews[i].get('asin'))\n",
    "        test_reviewer_id.append(all_reviews[i].get('reviewerID'))\n",
    "        test_unixreviewtime.append(all_reviews[i].get('unixReviewTime'))\n",
    "\n",
    "clean_text_test=preprocess_data(test_doc_list)\n",
    "\n",
    "# Create prediction array\n",
    "prediction = grid_DT.predict(clean_text_test)\n",
    "\n",
    "# Create output file\n",
    "dat = np.column_stack((test_prod_id, test_reviewer_id, prediction.astype(int), test_unixreviewtime))\n",
    "np.savetxt(\"Sports_and_Outdoors_Ratings_test.csv\", dat, delimiter=\",\", fmt=['%s','%s'], \n",
    "           header='asin,reviewerID,awesomeReview,unixReviewTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

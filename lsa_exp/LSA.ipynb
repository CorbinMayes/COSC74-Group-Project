{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_WARNED',\n",
       " '_WARNING',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'azure',\n",
       " 'bytebuffer',\n",
       " 'compression',\n",
       " 'concurrency',\n",
       " 'constants',\n",
       " 'doctools',\n",
       " 'gcs',\n",
       " 'hdfs',\n",
       " 'http',\n",
       " 'local_file',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'open',\n",
       " 'parse_uri',\n",
       " 'register_compressor',\n",
       " 's3',\n",
       " 's3_iter_bucket',\n",
       " 'smart_open',\n",
       " 'smart_open_lib',\n",
       " 'ssh',\n",
       " 'transport',\n",
       " 'utils',\n",
       " 'version',\n",
       " 'webhdfs']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smart_open\n",
    "dir(smart_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os.path\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import bigrams\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    " \n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was the original function I copied from some tutorial. It computes word coocurrence in \n",
    "# \"bigrams\" but I overwrite it below to compute coocurrence in 15-grams, similar to the \n",
    "# Huth paper.\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    " \n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    " \n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    " \n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    " \n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix, vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reproduces the 15-gram, log transformed zscore coocurrence matrices of Huth et. al\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create 15-grams from all words in corpus\n",
    "    n_grams = list(nltk.everygrams(corpus,min_len=15,max_len=15)) \n",
    "    \n",
    "    m = np.zeros([len(vocab),len(n_grams)]);\n",
    "    for i,word1 in enumerate(vocab):\n",
    "        for j,this_n_gram in enumerate(n_grams):\n",
    "            if word1 in this_n_gram:\n",
    "                 m[i,j]=1\n",
    "\n",
    "    co_occurrence_matrix = np.matrix(m)*np.matrix(np.transpose(m))\n",
    "\n",
    "    wordFreq = np.sum(m,axis=1)\n",
    "    frqtSrt = np.argsort(wordFreq)\n",
    "    frqtSrt = frqtSrt[::-1]\n",
    "    most_common = frqtSrt[0:985]\n",
    "    \n",
    "    co_occurrence_matrix = co_occurrence_matrix.take(most_common,axis=1)\n",
    "    \n",
    "    # normalize incidence\n",
    "    #incidence = np.sum(m,axis=1)\n",
    "    #a,b = np.shape(m)\n",
    "    #from numpy import matlib as ml\n",
    "    #incidence = ml.repmat(incidence,n=1,m=a)\n",
    "    #co_occurrence_matrix = co_occurrence_matrix/(incidence*np.transpose(incidence))\n",
    "    co_occurrence_matrix = np.log(1 + co_occurrence_matrix)\n",
    "    from scipy import stats as sts\n",
    "    co_occurrence_matrix = sts.zscore(sts.zscore(co_occurrence_matrix,axis=0),axis=1)\n",
    "\n",
    "    vocab_tuple = tuple(vocab_index)\n",
    "    basis_index = [];\n",
    "    for i in range(0,len(most_common)):\n",
    "        basis_index.append(vocab_tuple[most_common[i]])\n",
    "    \n",
    "    return co_occurrence_matrix, vocab_index, basis_index\n",
    "#plt.matshow(m)\n",
    "#idx = list(range(0,len(vocab),10))\n",
    "#plt.gca().set_xticks(idx)\n",
    "#plt.gca().set_xticklabels([vocab[i] for i in idx],rotation=90)\n",
    "#plt.gca().set_yticks(idx)\n",
    "#plt.gca().set_yticklabels([vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file_name):\n",
    "    \"\"\"\n",
    "    Input  : path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output : list of paragraphs/documents and\n",
    "             title(initial 100 words considred as title of document)\n",
    "    \"\"\"\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 4551\n"
     ]
    }
   ],
   "source": [
    "# LSA Model\n",
    "#number_of_topics=7\n",
    "#words=10\n",
    "document_list,titles=load_data(\"\",\"articles.csv\")\n",
    "clean_text=preprocess_data(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() got an unexpected keyword argument 'keepdims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b646d8cb8725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_co_occurrence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-ea1043275652>\u001b[0m in \u001b[0;36mgenerate_co_occurrence_matrix\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mco_occurrence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mco_occurrence_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mco_occurrence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_occurrence_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mvocab_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlclass/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mzscore\u001b[0;34m(a, axis, ddof, nan_policy)\u001b[0m\n\u001b[1;32m   2495\u001b[0m         \u001b[0msstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m         \u001b[0mmns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2498\u001b[0m         \u001b[0msstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() got an unexpected keyword argument 'keepdims'"
     ]
    }
   ],
   "source": [
    "data = list(itertools.chain.from_iterable(clean_text))\n",
    "matrix, vocab_index, basis_index = generate_co_occurrence_matrix(data[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get highest frequency words (they look better when plotted)\n",
    "idx = []\n",
    "for i in range(0,len(matrix)):\n",
    "    m = np.mean(np.abs(matrix[i]))\n",
    "    if m > np.power(float(10),-0.05):\n",
    "        idx.append(i) \n",
    "\n",
    "# I picked an arbitrary threshold above, so let's see how many words it picks.\n",
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tuple = tuple(vocab_index)\n",
    "subvocab = [];\n",
    "for i in range(0,len(idx)):\n",
    "    subvocab.append(vocab_tuple[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.matshow(matrix.take(list(range(0,len(idx))),axis=0).take(list(range(0,len(idx))),axis=1))\n",
    "\n",
    "plt.gca().set_xticks(range(0,len(idx)))\n",
    "plt.gca().set_xticklabels(list(basis_index[0:len(idx)]), rotation=90, fontsize=14)\n",
    "\n",
    "plt.gca().set_yticks(range(0,len(idx)))\n",
    "plt.gca().set_yticklabels(list(vocab_tuple[0:len(idx)]), fontsize=14)\n",
    "f = plt.gcf()\n",
    "f.colorbar(plt.gci())\n",
    "f.set_size_inches(11,9)\n",
    "\n",
    "p = plt.matshow(matrix)\n",
    "plt.gca().set_xticks(range(0,len(basis_index),100))\n",
    "plt.gca().set_xticklabels(list(basis_index[0:len(basis_index):100]), rotation=90, fontsize=14)\n",
    "\n",
    "plt.gca().set_yticks(range(0,len(vocab_tuple),500))\n",
    "plt.gca().set_yticklabels(list(vocab_tuple[0:len(vocab_tuple):500]), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

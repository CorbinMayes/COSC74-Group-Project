{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script works at a higher level of abstraction than the word-based semantic space. Instead of using similarity between word coocurrence to compute it's space, the space is defined based on the similarity of \"documents\" (aka reviews). The advantage of this approach is that each review ends up represented as a vector, eliminating the need to categorize words. The implementation is less transparent though and instead demonstrates the usage of gensim tools.\n",
    "\n",
    "This script also implements naive bayes. Classification accuracy is around 66% in a hold out sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import bigrams\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_list = []\n",
    "for line in open('../data/Sports_and_Outdoors_Reviews_training.json', 'r'):\n",
    "    this_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "doc_list = []\n",
    "rating = []\n",
    "for i in range(0,len(this_list)):\n",
    "    if this_list[i].get('reviewText'): #not all reviews have text\n",
    "        rating.append(this_list[i].get('overall'))\n",
    "        doc_list.append(this_list[i].get('reviewText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=preprocess_data(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel as lsi\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "n_doc = 2000\n",
    "\n",
    "# mapping of words to numbers\n",
    "this_dict = Dictionary(clean_text[0:n_doc])\n",
    "\n",
    "# transform corpus (clean_text) into a 2d array word counts\n",
    "bow_corpus = [this_dict.doc2bow(text) for text in clean_text[0:n_doc]]\n",
    "\n",
    "# construct a semantic space based on document-topic similarity\n",
    "semSpace = lsi(bow_corpus, id2word=this_dict, num_topics=300)\n",
    "\n",
    "# convert documents into vectors in topic feature space\n",
    "vectors = semSpace[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from TransformedCorpus datatype to numpy doc x topic array\n",
    "from gensim import matutils\n",
    "all_topics_csr = matutils.corpus2csc(vectors)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1249 points : 435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "lbl = [i >4.5 for i in rating]\n",
    "\n",
    "n_train = int(np.floor(n_doc*0.75))\n",
    "n_test = int(n_doc - n_train)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "X_train = all_topics_numpy[0:n_train]\n",
    "y_train = lbl[0:n_train]\n",
    "X_test = all_topics_numpy[(n_train+1):(n_train+n_test)]\n",
    "y_test = lbl[(n_train+1):(n_train+n_test)]\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

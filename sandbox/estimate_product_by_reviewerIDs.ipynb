{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of naive bayes classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_reviews(text, asins, ratings):\n",
    "    # this script uses an algorithm that requires sorted asins, so enforce it\n",
    "    # introduces overhead if arrays are already sorted (as they are by default)\n",
    "    # but makes for a more reliable function\n",
    "    dat = zip(text, asins, ratings)\n",
    "    dat = sorted(dat, key=lambda id: id[1]) \n",
    "    dat = [[i for i,j,k in dat], [j for i,j,k in dat], [k for i,j,k in dat]]\n",
    "    text = dat[0]\n",
    "    asins = dat[1]\n",
    "    ratings = dat[2]\n",
    "    \n",
    "    products = [asins[0]]\n",
    "    combined_text = [text[0]]\n",
    "    average_rating = []\n",
    "    total_rating = ratings[0]\n",
    "    count = 1\n",
    "    \n",
    "    #combine all the summaries into a single text and avg the review ratings for each product\n",
    "    for i in range(1, len(asins)):\n",
    "        last_element_index = len(products) - 1\n",
    "        if(asins[i] == products[last_element_index]):\n",
    "            combined_text[last_element_index] = combined_text[last_element_index] + text[i]\n",
    "            total_rating += ratings[i]\n",
    "            count += 1\n",
    "        else:\n",
    "            average_rating.append(total_rating/count)\n",
    "            products.append(asins[i])\n",
    "            combined_text.append(text[i])\n",
    "            total_rating = ratings[i]\n",
    "            count = 1\n",
    "    average_rating.append(total_rating/count)\n",
    "    \n",
    "    return (combined_text, products, average_rating)\n",
    "\n",
    "# searches for first match to target in dat, beginning\n",
    "# search at start_offset\n",
    "# useful for searching sorted lists.\n",
    "def linearSearch(dat, target, start_offset=0):\n",
    "    for i in range(start_offset, len(dat)):\n",
    "        if target == dat[i]:\n",
    "            return i\n",
    "\n",
    "# returns elements of list1 not in list2.\n",
    "#\n",
    "# list1 - n x 1 array (possibly of tuples)\n",
    "# list2 - m x 1 array of labels\n",
    "# col - col of list1 tuples to match to list 2\n",
    "#\n",
    "# returns: list1 without elements also in list2\n",
    "def getUnique(list1, list2, col=0):\n",
    "    list1 = sorted(list1, key=lambda id: id[col])\n",
    "    list2 = sorted(np.unique(list2))\n",
    "    list1_target = np.array([i[col] for i in list1])\n",
    "    \n",
    "    idx = 0\n",
    "    nonunique = []\n",
    "    for elem in list2:\n",
    "        new_idx = linearSearch(list1_target, elem, idx)\n",
    "        \n",
    "        if new_idx:\n",
    "            idx = new_idx\n",
    "            while idx < len(list1_target) and list1_target[idx] == elem:\n",
    "                nonunique.append(idx)\n",
    "                idx += 1\n",
    "    \n",
    "    nonunique.reverse()\n",
    "    [list1.pop(i) for i in nonunique]\n",
    "    \n",
    "    return list1\n",
    "    \n",
    "\n",
    "# takes n x 1 vectors of prsn_ratings and matching prsn_id,\n",
    "# and an m x 1 (n >= m) vector of uniq_prsn_ids for whom we\n",
    "# want to get average X. Does not preserve order.\n",
    "# returns new uniq_lbls corresponding to order of avg_X\n",
    "# O( n log(n) )\n",
    "def avgByLbl(X, lbls):\n",
    "    uniq_lbls = np.unique(lbls)\n",
    "    \n",
    "    # sort data for efficient averaging\n",
    "    dat = sorted(list(zip(X,lbls)), key=lambda id: id[1])\n",
    "    dat = [[i for i,j in dat], [j for i,j in dat]]\n",
    "    X = dat[0]\n",
    "    lbls = dat[1]\n",
    "    \n",
    "    uniq_lbls = sorted(uniq_lbls)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # use an averaging algorithm that assumes sorted entries\n",
    "    # for both search targets and search list.\n",
    "    # this algorithm never traverses the same element of the\n",
    "    # search list twice, but carries the overhead of a pre-\n",
    "    # sorted target list and search list. Thankfully those\n",
    "    # can use the O(n log(n)) python sort implementation\n",
    "    avg_X = np.zeros(len(uniq_lbls))\n",
    "    idx = 0\n",
    "    for i,this_id in enumerate(uniq_lbls):\n",
    "        idx = linearSearch(lbls, this_id, idx)\n",
    "        n = 0.0\n",
    "        while idx < len(lbls) and lbls[idx] == this_id:\n",
    "            avg_X[i] += X[idx]\n",
    "            n += 1.0\n",
    "            idx += 1\n",
    "        avg_X[i] /= n\n",
    "            \n",
    "    t1 = time.time()\n",
    "    print(t1-t0)\n",
    "    return avg_X, uniq_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# this classifier will predict each rating based on whatever the subjects average rating is.\n",
    "# wrap this in a GroupKFold cross validator to make predictions based on ratings on other products\n",
    "# and then average over ratings for each product outside the CV loop to get product\n",
    "# ratings\n",
    "class subjProfileClf(BaseEstimator):\n",
    "    # asumes X is tuple (prsn_id, prsn_rating, prod_id)\n",
    "    def fit(self, X, y=None):\n",
    "        prsn_id = [i[0] for i in X]\n",
    "        prsn_rating = [i[1] for i in X]\n",
    "        \n",
    "        avg_rating, uniq_prsn_id = avgByLbl(prsn_rating, prsn_id)\n",
    "        self.profile = dict(zip(uniq_prsn_id, avg_rating))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        prsn_id = [i[0] for i in X]\n",
    "        prod_id = [i[2] for i in X]\n",
    "        \n",
    "        meanValue = np.mean(np.array(list(self.profile.values())))\n",
    "        \n",
    "        y = [meanValue]*len(prod_id)\n",
    "        for i in range(0,len(prod_id)):\n",
    "            y[i] = np.mean([self.profile[rid] for rid in prsn_id if prsn_id in profile])\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "#json_dat = json_dat[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "reviewerID = []\n",
    "summary = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    rating.append(json_dat[i].get('overall'))\n",
    "    summary.append([])\n",
    "    reviewerID.append(json_dat[i].get('reviewerID'))\n",
    "    prod_id.append(json_dat[i].get('asin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.605073928833008\n"
     ]
    }
   ],
   "source": [
    "combined_text, products, average_rating = combine_reviews([[rid] for rid in reviewerID], prod_id, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.309006452560425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "prsn_asin = []\n",
    "prsn_id = []\n",
    "prsn_rating = []\n",
    "with open('../data/Sports_and_Outdoors_Ratings_training.csv') as file:\n",
    "    reader = pd.read_csv(file, delimiter=',')\n",
    "    prsn_rating = np.array([item[1] for item in reader['overall'].items()])\n",
    "    prsn_id = np.array([item[1] for item in reader['reviewerID'].items()])\n",
    "    prsn_asin = np.array([item[1] for item in reader['asin'].items()])\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# only retain ratings for products not in training json\n",
    "# crude way of ensuring we're not double dipping when testing the\n",
    "# reviewer model on training dataset product ratings\n",
    "dat = list(zip(prsn_asin, prsn_rating, prsn_id))\n",
    "dat = getUnique(dat, products)\n",
    "dat = [[i for i,j,k in dat], [j for i,j,k in dat], [k for i,j,k in dat]] \n",
    "prsn_asin = dat[0]\n",
    "prsn_rating = dat[1]\n",
    "prsn_id = dat[2]\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "len(prsn_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.928819417953491\n",
      "32.49006271362305\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "avg_rating, uniq_prsn_id = avgByLbl(prsn_rating, prsn_id)\n",
    "profile = dict(zip(uniq_prsn_id,avg_rating))\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "import gc\n",
    "\n",
    "del summary\n",
    "del review\n",
    "del json_dat\n",
    "del val_dat\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/bope9760/.conda/envs/mlclass/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/projects/bope9760/.conda/envs/mlclass/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# compute a predicted rating by averaging over the expected\n",
    "# ratings of all people who reviewed the product.\n",
    "pred_rating = []\n",
    "for text in total_text:\n",
    "    pred_rating.append(np.mean([profile[rid] for rid in text if rid in profile]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize training data\n",
    "train_lbls = np.array(avg_ratings) >= 4.5\n",
    "train_text = total_text\n",
    "\n",
    "pred_rating = np.array(pred_rating)\n",
    "idx = np.isfinite(pred_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True recall is sensitivity, false recall is specificity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.65      0.61     34937\n",
      "        True       0.56      0.49      0.52     32301\n",
      "\n",
      "    accuracy                           0.57     67238\n",
      "   macro avg       0.57      0.57      0.57     67238\n",
      "weighted avg       0.57      0.57      0.57     67238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(train_lbls[idx], pred_rating[idx] > 4.5)\n",
    "print('True recall is sensitivity, false recall is specificity')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True recall is sensitivity, false recall is specificity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.52      0.59      0.55     34937\n",
      "        True       0.48      0.42      0.45     32301\n",
      "\n",
      "    accuracy                           0.50     67238\n",
      "   macro avg       0.50      0.50      0.50     67238\n",
      "weighted avg       0.50      0.50      0.50     67238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(train_lbls[idx], np.random.permutation(pred_rating[idx] > 4.5))\n",
    "print('True recall is sensitivity, false recall is specificity')\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

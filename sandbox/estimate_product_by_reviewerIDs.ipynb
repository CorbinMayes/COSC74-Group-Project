{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of naive bayes classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes an array of lists as input, product labels, uniq_labels, and ratings,\n",
    "# and merges lists with matching labels among labels uniq_labels, averages\n",
    "# reviews belonging to the same, returns merged lists, and averaged ratings\n",
    "# uniq_labels should typically be np.unique(product labels), however \n",
    "# the option of specifying a subset is useful for parallelization to allow\n",
    "# different subsets to be processed by different engines\n",
    "def combine_reviews(review_text, asins, uniq_asins, ratings):\n",
    "    # cast to array for easier indexing\n",
    "    review_text = np.array(review_text, dtype=object)\n",
    "    ratings = np.array(ratings)\n",
    "    asins = np.array(asins)\n",
    "    \n",
    "    #combine all the summaries into a single text and avg the review ratings for each product\n",
    "    product_text = []\n",
    "    avg_ratings = []\n",
    "    for this_asin in uniq_asins:\n",
    "        asin_idx = np.where(this_asin == asins)\n",
    "        this_product_text = np.concatenate([text for text in review_text[asin_idx]])\n",
    "        product_text.append(this_product_text)\n",
    "        avg_ratings.append(np.mean(ratings[asin_idx]))\n",
    "    \n",
    "    return (product_text, avg_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "#json_dat = json_dat[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "reviewerID = []\n",
    "summary = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    rating.append(json_dat[i].get('overall'))\n",
    "    summary.append([])\n",
    "    reviewerID.append(json_dat[i].get('reviewerID'))\n",
    "    prod_id.append(json_dat[i].get('asin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378.04977083206177\n"
     ]
    }
   ],
   "source": [
    "# takes ~96 CPU minutes\n",
    "\n",
    "# this cell runs things in parallel. make sure to start an \n",
    "# ipython cluster from the notebook dashboard's IPython Cluster\n",
    "# tab before running\n",
    "import ipyparallel as ipp\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "dview.execute('from nltk.tokenize import RegexpTokenizer;' +\n",
    "              'from nltk.corpus import stopwords; ' + \n",
    "              'from nltk.stem.porter import PorterStemmer;' +\n",
    "              'import numpy as np;')\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "# combine reviewerID\n",
    "dview.push(dict(reviewerID=[[rid] for rid in reviewerID], combine_reviews=combine_reviews,\n",
    "               rating=rating, prod_id=prod_id))\n",
    "uniq_prod_id = np.unique(prod_id)\n",
    "dview.scatter('uniq_prod_id', uniq_prod_id) # partition target labels\n",
    "\n",
    "%px total_text, avg_ratings = combine_reviews(reviewerID, prod_id, uniq_prod_id, rating)\n",
    "total_text = dview.gather('total_text').get()\n",
    "avg_ratings = dview.gather('avg_ratings').get()\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel_avg_ratings = avg_ratings\n",
    "#parallel_total_text = total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment this box to run serially\n",
    "\n",
    "#uniq_prod_id = np.unique(prod_id)\n",
    "#reviewerID, avg_ratings = combine_reviews([[rid] for rid in reviewerID], prod_id, uniq_prod_id, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(parallel_avg_ratings == avg_ratings)\n",
    "#all([all(parallel_total_text[i] == text) for i, text in enumerate(total_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "prsn_asin = []\n",
    "prsn_id = []\n",
    "prsn_rating = []\n",
    "with open('../data/Sports_and_Outdoors_Ratings_training.csv') as file:\n",
    "    reader = pd.read_csv(file, delimiter=',')\n",
    "    prsn_rating = np.array([item[1] for item in reader['overall'].items()])\n",
    "    prsn_id = np.array([item[1] for item in reader['reviewerID'].items()])\n",
    "    prsn_asin = np.array([item[1] for item in reader['asin'].items()])\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "# only retain ratings for products not in training json\n",
    "# crude way of ensuring were not double dipping when testing the\n",
    "# reviewer model on training dataset product ratings\n",
    "idx = [i for i, pid in enumerate(prsn_asin) if pid not in uniq_prod_id]\n",
    "prsn_rating = prsn_rating[idx]\n",
    "prsn_id = prsn_id[idx]\n",
    "prsn_asin = prsn_asin[idx]\n",
    "\n",
    "len(prsn_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "profile = dict(zip(np.unique(prsn_id), [None]*len(np.unique(prsn_id))))\n",
    "for prsn in profile:\n",
    "    profile[prsn] = np.mean(prsn_rating[prsn == prsn_id])\n",
    "    \n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n x 1 vectors of prsn_ratings and matching prsn_id,\n",
    "# and an m x 1 (n >= m) vector of uniq_prsn_ids for whom we\n",
    "# want to get average ratings\n",
    "def mkProfile(prsn_ratings, prsn_ids, uniq_prsn_id):\n",
    "    avg_rating = []\n",
    "    for this_id in uniq_prsn_id:\n",
    "        avg_rating.append(np.mean(prsn_ratings[this_id == prsn_ids]))\n",
    "        \n",
    "    return avg_rating, uniq_prsn_id\n",
    "    \n",
    "uniq_prsn_id = np.unique(prsn_id)\n",
    "\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "dview.execute('import numpy as np;')\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "# combine reviewerID\n",
    "dview.push(dict(prsn_rating=prsn_rating, prsn_id=prsn_id, mkProfile=mkProfile))\n",
    "dview.scatter('uniq_prsn_id', uniq_prsn_id) # partition target labels\n",
    "%px avg_rating, uniq_prsn_id = mkProfile(prsn_rating, prsn_id, uniq_prsn_id)\n",
    "\n",
    "profile = dict(zip(dview.gather('uniq_prsn_id').get(), dview.gather('avg_rating').get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3866623"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del summary\n",
    "del review\n",
    "del json_dat\n",
    "del val_dat\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a predicted rating by averaging over the expected\n",
    "# ratings of all people who reviewed the product.\n",
    "pred_rating = []\n",
    "for text in total_text:\n",
    "    pred_rating.append(np.mean([profile[rid] for rid in text if rid in profile]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize training data\n",
    "train_lbls = np.array(avg_ratings) >= 4.5\n",
    "train_text = total_text\n",
    "\n",
    "pred_rating = np.array(pred_rating)\n",
    "idx = np.isfinite(pred_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(train_lbls[idx], pred_rating[idx] > 4.5)\n",
    "print('True recall is sensitivity, false recall is specificity')\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

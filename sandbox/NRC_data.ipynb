{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from spellchecker import SpellChecker\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNRC(filename):\n",
    "    \"\"\" Reads the NRC lexicon into a dictionary.\n",
    "    \"\"\"\n",
    "    wordToEmotions = dict()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    count = 0\n",
    "    with open(filename, 'r') as fp:\n",
    "        # Loop through lines\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            words = line.split('\\t')\n",
    "            if len(words) != 3:\n",
    "                continue\n",
    "            # Stem word\n",
    "            word = p_stemmer.stem(words[0])\n",
    "            val = int(line[-1:])\n",
    "            # Store the emotions associated with the word\n",
    "            if count == 0:\n",
    "                wordToEmotions[word] = np.array([val])\n",
    "            else:\n",
    "                wordToEmotions[word] = np.append(wordToEmotions[word],val)\n",
    "                \n",
    "            count = (count + 1)%10\n",
    "    return wordToEmotions\n",
    "\n",
    "lexicon = readNRC(\"../data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "emotionList = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(lexicon['angel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = json_dat[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "sp = SpellChecker()\n",
    "\n",
    "# Returns a list with percentage of words which conveyed [anger, anticipation, ... , trust]\n",
    "def getEmotions(words, lexicon):\n",
    "    emotionCount = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "    emotionWords = 0\n",
    "    for word in words:\n",
    "#         print(f'{word}\\r', end='')\n",
    "        # Stem each word\n",
    "        word = p_stemmer.stem(sp.correction(word))\n",
    "        # Sum the emotions\n",
    "        if word in lexicon.keys():\n",
    "            emotionWords += 1\n",
    "            emotionCount = emotionCount + lexicon[word]\n",
    "        \n",
    "    emotionCount[emotionCount == 0] = 0.1\n",
    "    # Avg over all words\n",
    "    if emotionWords > 0:\n",
    "        emotionCount = emotionCount / emotionWords\n",
    "    \n",
    "    return emotionCount\n",
    "\n",
    "string1 = sampleData[45].get('summary')\n",
    "print(getEmotions(simple_preprocess(string1, deacc = True), lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes 0.0020008087158203125,Takes 0.004990339279174805,Takes 0.007004261016845703,Takes 0.0009946823120117188,Takes 0.0,Takes 0.0010080337524414062,Takes 0.005991458892822266,Takes 0.0070040225982666016,Takes 0.0,Takes 0.006996870040893555,Takes 0.001999378204345703,Takes 0.001003265380859375,Takes 0.009999275207519531,Takes 0.0010046958923339844,Takes 0.01299905776977539,Takes 0.005001544952392578,Takes 0.0029990673065185547,Takes 0.002996683120727539,Takes 0.001004934310913086,Takes 0.0029990673065185547,Takes 0.000997781753540039,Takes 0.0020017623901367188,Takes 0.02099442481994629,Takes 0.0010027885437011719,Takes 0.004006624221801758,Takes 0.001993894577026367,Takes 0.010000944137573242,Takes 0.009007692337036133,Takes 1.3049616813659668,Takes 0.006002664566040039,"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in range(30):\n",
    "    ra = randint(0, len(sampleData))\n",
    "    time0 = time.time()\n",
    "    getEmotions(simple_preprocess(sampleData[ra].get('reviewText')), lexicon)\n",
    "    time1 = time.time()\n",
    "    print(f'Takes {time1-time0},',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18%]\r"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "emotions = []\n",
    "for i in range(len(sampleData)):\n",
    "    currJson = sampleData[i]\n",
    "    if(not currJson.get('summary')):\n",
    "        continue\n",
    "    # Get score\n",
    "    scores.append(currJson.get('overall'))\n",
    "    # Get summary words and do analysis\n",
    "    words = simple_preprocess(currJson.get('summary'), deacc=True)\n",
    "    currEmotions = getEmotions(words, lexicon)\n",
    "    emotions.append(currEmotions)\n",
    "    \n",
    "    print('[%d%%]\\r' % (100*(i+1)/len(sampleData)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n",
      "[array([0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ])]\n"
     ]
    }
   ],
   "source": [
    "train_lbls = np.array(scores) >= 4.5\n",
    "np_emotions = np.array(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_emotions, train_lbls, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Make the classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6639579947493437\n",
      "Precision: 0.687624183934147\n",
      "Recall: 0.9086646661665416\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

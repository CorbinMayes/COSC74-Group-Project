{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from spellchecker import SpellChecker\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NRC lexicon into a dictionary\n",
    "def readNRC(filename):\n",
    "    \"\"\" Reads the NRC lexicon into a dictionary.\n",
    "    \"\"\"\n",
    "    wordToEmotions = dict()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    count = 0\n",
    "    with open(filename, 'r') as fp:\n",
    "        # Loop through lines\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            words = line.split('\\t')\n",
    "            if len(words) != 3:\n",
    "                continue\n",
    "            # Stem word\n",
    "            word = p_stemmer.stem(words[0])\n",
    "#             word = words[0]\n",
    "            val = int(line[-1:])\n",
    "            # Store the emotions associated with the word\n",
    "            if count == 0:\n",
    "                wordToEmotions[word] = np.array([val])\n",
    "            else:\n",
    "                wordToEmotions[word] = np.append(wordToEmotions[word],val)\n",
    "                \n",
    "            count = (count + 1)%10\n",
    "    return wordToEmotions\n",
    "\n",
    "lexicon = readNRC(\"../data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "emotionList = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anger', 0),\n",
       " ('anticipation', 0),\n",
       " ('disgust', 0),\n",
       " ('fear', 0),\n",
       " ('joy', 0),\n",
       " ('negative', 0),\n",
       " ('positive', 0),\n",
       " ('sadness', 0),\n",
       " ('surprise', 0),\n",
       " ('trust', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "p_stemmer = PorterStemmer()\n",
    "list(zip(emotionList, lexicon[p_stemmer.stem('amazement')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "sp = SpellChecker()\n",
    "\n",
    "# Returns a list with percentage of words which conveyed [anger, anticipation, ... , trust]\n",
    "def getEmotions(text, lexicon):\n",
    "    emotionCount = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "    emotionWords = 0\n",
    "    for word in simple_preprocess(text,deacc=True):\n",
    "        # Stem each word\n",
    "        word = p_stemmer.stem(word)\n",
    "        # Sum the emotions\n",
    "        if word in lexicon.keys():\n",
    "            emotionWords += 1\n",
    "            emotionCount = emotionCount + lexicon[word]\n",
    "        \n",
    "    # Avg over all words\n",
    "    if emotionWords > 0:\n",
    "        emotionCount = emotionCount / emotionWords\n",
    "    \n",
    "    return emotionCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%]\r"
     ]
    }
   ],
   "source": [
    "# Get the sample data\n",
    "num = 10000\n",
    "sampleData = []\n",
    "for i in range(num):\n",
    "    index = randint(0,len(json_dat))\n",
    "    sampleData.append(json_dat[index])\n",
    "\n",
    "# Create a vector for each data point\n",
    "scores = []\n",
    "emotions = []\n",
    "for i in range(len(sampleData)):    \n",
    "    print('[%d%%]\\r' % (100*(i+1)/len(sampleData)), end='')\n",
    "    currJson = sampleData[i]\n",
    "    if(not currJson.get('reviewText')):\n",
    "        continue\n",
    "    # Get score\n",
    "    scores.append(currJson.get('overall'))\n",
    "    # Get summary words and do analysis\n",
    "    currEmotions = getEmotions(currJson.get('reviewText'), lexicon)\n",
    "    emotions.append(currEmotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = np.array(scores) >= 4.5\n",
    "np_emotions = np.array(emotions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_emotions, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.677\n",
      "Precision: 0.6786437246963563\n",
      "Recall: 0.9918639053254438\n",
      "F1-score: 0.8058894230769231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.54      0.02      0.04       648\n",
      "        True       0.68      0.99      0.81      1352\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.61      0.51      0.42      2000\n",
      "weighted avg       0.63      0.68      0.56      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Make the classifier\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "# np.random.shuffle(y_train)\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "# Model F1-score\n",
    "print(\"F1-score:\",metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "# Report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.604\n",
      "Precision: 0.6718946047678795\n",
      "Recall: 0.7992537313432836\n",
      "F1-score: 0.7300613496932515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.34      0.21      0.26       660\n",
      "        True       0.67      0.80      0.73      1340\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.50      0.50      0.49      2000\n",
      "weighted avg       0.56      0.60      0.57      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=8)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_pred = neigh.predict(X_test)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "# Model F1-score\n",
    "print(\"F1-score:\",metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "# Report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

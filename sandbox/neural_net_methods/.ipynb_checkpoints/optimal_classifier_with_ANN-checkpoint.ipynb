{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script uses neural nets in the following places\n",
    "* for part of speech classification in lemmatization\n",
    "* word2Vec semantic embeddings\n",
    "\n",
    "Ideally we can do several things here:\n",
    "* use existing word embeddings to classify summary text\n",
    "* use more sentiment analysis to classify review text\n",
    "* have a classifier that enforces True iff review text is true and summary text is true (true = awesome)\n",
    "* assign sentiments to nouns, identify relevant/irrelevant nouns, and classify based on their sentiments\n",
    "\n",
    "sentiment analysis alg idea:\n",
    "find adj-noun pairs\n",
    "train classifier on training set based on noun semantics. Identify relevant nouns by a threshold over the SVM coefficients\n",
    "train classifer on training set based on verb semantics.\n",
    "predict outcome based on verbs attached to relevant nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../libraries/\")\n",
    "from selector import split_data\n",
    "from semanticClassifiers import docTopTransformer2,bigramsPhraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "json_dat = json_dat[0:141792]\n",
    "del val_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists with one review per elem\n",
    "summary = []\n",
    "review = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    if json_dat[i].get('summary'): #not all reviews have text\n",
    "        rating.append(json_dat[i].get('overall'))\n",
    "        summary.append(json_dat[i].get('summary'))\n",
    "        prod_id.append(json_dat[i].get('asin'))\n",
    "        \n",
    "del json_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the get_wordnet_pos() function relies on a neural method, \n",
    "# and preprocess_data_lemmatize therefore does too. Implemented\n",
    "# here for some benchmarking only\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def preprocess_data_lemmatize(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create spell checker\n",
    "    sc = SpellChecker()\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # create spell checker\n",
    "    sp = SpellChecker()\n",
    "    # Create lemmatizer class\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # autocorrect spelling\n",
    "        tokens = [sp.correction(this_tok) for this_tok in tokens]\n",
    "        # get part of speech tag\n",
    "        this_pos = [tag[1] for tag in nltk.pos_tag(tokens)]\n",
    "        # remove stop words from tokens, also return associatd tags\n",
    "        stopped_tokens = [(this_tok, this_pos[i]) for i,this_tok in enumerate(tokens)\n",
    "                          if not this_tok in en_stop]        \n",
    "        # lemmatize tokens\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(this_tok[0], pos=get_wordnet_pos(this_tok[1]))\n",
    "                             for this_tok in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append([(this_tok, this_pos[i]) for i,this_tok in enumerate(lemmatized_tokens)])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "awesome = np.array(rating) >= 4.5\n",
    "lem_summary_text = preprocess_data_lemmatize(summary[0:18000])\n",
    "t1 = time.time()\n",
    "#del summary\n",
    "#del review\n",
    "\n",
    "print(t1-t0)\n",
    "lem_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(16)\n",
    "try:\n",
    "    x = pool.map(preprocess_data_lemmatize, [summary])\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "lem_summary_text = x[0]\n",
    "awesome = np.array([x >= 4.5 for x in rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much better than plastic caps.\n",
      "[('much', 'RB'), ('good', 'JJR'), ('plastic', 'IN'), ('cap', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(summary[i])\n",
    "print(lem_summary_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# expects corpus of tuples as input, (word, part-of-speach)\n",
    "# nltk.pos_tagger has the following pos_tag convention\n",
    "# N* - noun\n",
    "# J* - adjective\n",
    "# V* - verb\n",
    "# R* - adverb\n",
    "# use nltk.help.upenn_tagset('CD') to reverse lookup tags, \n",
    "# where CD is whatever your tag is\n",
    "class posDoc(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, pos_tag=\"N\"):\n",
    "        self.pos_tag = pos_tag\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = []\n",
    "        for doc in X:\n",
    "            this_doc = [word[0] for word in doc \n",
    "                        if word[1][:len(self.pos_tag)] == self.pos_tag]\n",
    "            this_doc.append('null') # prior value to avoid null set return\n",
    "            X_.append(this_doc)\n",
    "        return X_\n",
    "    \n",
    "# see here regarding this function:\n",
    "# https://stackoverflow.com/questions/41881086/valueerror-with-scikit-learn-plsregression-when-used-in-pipeline\n",
    "class PLSRegressionWrapper(PLSRegression):\n",
    "    def transform(self, X):\n",
    "        return super().transform(X)\n",
    "\n",
    "    def fit_transform(self, X, Y):\n",
    "        return self.fit(X,Y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "    \n",
    "class getLatentWord(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = KeyedVectors.load_word2vec_format('../../../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        cumWord = []\n",
    "        for review in X:\n",
    "            review_word_vec = [self.model[word] for word in review \n",
    "                                   if self.model.vocab.__contains__(word)]\n",
    "            review_word_vec.append(self.model['null']) # works as prior so that we can score empty reviews\n",
    "            cumWord.append(np.sum(review_word_vec, axis=0))\n",
    "        return cumWord\n",
    "    \n",
    "            \n",
    "# assumes words are already in latent representation\n",
    "class getLatentTopics(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    # takes words as input\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # train a document-topic model        \n",
    "        self.pca = PCA(n_components = 100)\n",
    "        self.pca.fit(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        return self.pca.transform(X)\n",
    "    \n",
    "class docTopTransformer(BaseEstimator):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.31337714195251\n",
      "0.758697509765625\n"
     ]
    }
   ],
   "source": [
    "# fit some example models to see what's being misclassified or correctly classified\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "    \n",
    "from sklearn.pipeline import Pipeline\n",
    "from copy import deepcopy\n",
    "\n",
    "idx = np.random.permutation([x for x in range(0, len(lem_summary_text))])\n",
    "sample_idx = idx[0:np.power(2,16)]\n",
    "\n",
    "train_lbls = awesome[sample_idx]\n",
    "this_prod_id = [prod_id[i] for i in sample_idx]\n",
    "lem_train_text = deepcopy([lem_summary_text[i] for i in sample_idx])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "cv = gkf.split(lem_train_text, train_lbls, groups=this_prod_id)\n",
    "n_dim = 2\n",
    "\n",
    "nounSpace = Pipeline([('getNoun', posDoc(pos_tag=\"N\")), ('doc2Top',docTopTransformer2(d=75)), \n",
    "                      ('dRedux', PLSRegressionWrapper(n_components=n_dim))])\n",
    "verbSpace = Pipeline([('getNoun', posDoc(pos_tag=\"V\")), ('doc2Top',docTopTransformer2(d=75)), \n",
    "                      ('dRedux', PLSRegressionWrapper(n_components=n_dim))])\n",
    "advSpace = Pipeline([('getNoun', posDoc(pos_tag=\"R\")), ('doc2Top',docTopTransformer2(d=75)), \n",
    "                      ('dRedux', PLSRegressionWrapper(n_components=n_dim))])\n",
    "adjSpace = Pipeline([('getNoun', posDoc(pos_tag=\"J\")), ('doc2Top',docTopTransformer2(d=75)), \n",
    "                      ('dRedux', PLSRegressionWrapper(n_components=n_dim))])\n",
    "numSpace = Pipeline([('getNoun', posDoc(pos_tag=\"CD\")), ('doc2Top',docTopTransformer2(d=10)), \n",
    "                      ('dRedux', PLSRegressionWrapper(n_components=n_dim))])\n",
    "featureExtract = FeatureUnion([('nouns', nounSpace), ('verbs', verbSpace), ('adv', advSpace), \n",
    "                               ('adj', adjSpace), ('numbers', numSpace)])\n",
    "\n",
    "clf = SVC(kernel='poly', degree=2)\n",
    "\n",
    "estimators = [('getFeatures', featureExtract), ('classify', clf)]\n",
    "semClf = Pipeline(estimators)\n",
    "\n",
    "t0 = time.time()\n",
    "predict = cross_val_predict(semClf, lem_train_text, train_lbls, cv=cv, n_jobs=5)\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)\n",
    "\n",
    "print(1 - np.sum(predict != train_lbls)/len(train_lbls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Text Prediction\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.31      0.45     21538\n",
      "        True       0.74      0.98      0.85     43998\n",
      "\n",
      "    accuracy                           0.76     65536\n",
      "   macro avg       0.81      0.64      0.65     65536\n",
      "weighted avg       0.79      0.76      0.72     65536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(train_lbls, predict)\n",
    "print('Summary Text Prediction')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [i for i in range(0,18000) if i not in sample_idx]\n",
    "#lem_train_text = deepcopy([lem_summary_text[i] for i in sample_idx])\n",
    "#doc2wordEmb = getLatentWord()\n",
    "semClf = semClf.fit(lem_train_text, train_lbls)\n",
    "\n",
    "test_lbls = awesome[test_idx]\n",
    "test_text = deepcopy([lem_summary_text[i] for i in test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = semClf.predict(test_text)\n",
    "print(1 - np.sum(predict2 != test_lbls)/len(test_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

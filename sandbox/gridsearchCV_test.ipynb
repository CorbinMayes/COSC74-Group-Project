{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script estimates the performance of linear SVM classification in a document-topic model's feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../libraries/\")\n",
    "from selector import split_data\n",
    "from semanticClassifiers import docTopTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data for 80/20 split\n",
    "# we won't use val_dat at all\n",
    "json_dat, val_dat = split_data('../data/Sports_and_Outdoors_Reviews_training.json', 80)\n",
    "json_dat = json_dat[0:50000] # use this for prototyping on smaller subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, doc_list, with one review per elem\n",
    "doc_list = []\n",
    "rating = []\n",
    "prod_id = []\n",
    "for i in range(0,len(json_dat)):\n",
    "    if json_dat[i].get('reviewText'): #not all reviews have text\n",
    "    #if json_dat[i].get('summary'): #not all reviews have summary text\n",
    "        rating.append(json_dat[i].get('overall'))\n",
    "        #doc_list.append(json_dat[i].get('summary'))\n",
    "        doc_list.append(json_dat[i].get('reviewText'))\n",
    "        prod_id.append(json_dat[i].get('asin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=preprocess_data(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import gensim.matutils as matutils\n",
    "\n",
    "# vectorize training data\n",
    "train_lbls = np.array(rating) >= 4.5\n",
    "train_text = clean_text\n",
    "\n",
    "# train a document-topic model        \n",
    "this_dict = Dictionary(train_text)\n",
    "\n",
    "# transform corpus (train) into a 2d array word counts (a 'bag of words')\n",
    "bow_train = [this_dict.doc2bow(text) for text in train_text]\n",
    "bow_train = np.transpose(matutils.corpus2dense(bow_train, len(this_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 26.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659.9721484184265\n"
     ]
    }
   ],
   "source": [
    "# estimate classifier accuracy\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# GroupKFold gives you a KFold partitioner that abides by\n",
    "# product labels so that products are only ever in a single\n",
    "# fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "cv = gkf.split(bow_train, train_lbls, groups=prod_id)\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "# initialize a transformer mapping from bow to latent semantic features\n",
    "bow2Top = docTopTransformer(this_dict=this_dict, d=300, distributed=False)\n",
    "\n",
    "params = {\n",
    "    'clf__base_estimator__max_depth': [3, 5, 7, 10, 15],\n",
    "    'clf__base_estimator__min_samples_leaf': [1],\n",
    "    'clf__base_estimator__max_leaf_nodes': [20],\n",
    "    'clf__n_estimators': [10],\n",
    "    'clf__max_samples': [0.1]\n",
    "}\n",
    "\n",
    "# pick a classifier\n",
    "baseClf = DecisionTreeClassifier()\n",
    "\n",
    "clf = BaggingClassifier(base_estimator=baseClf, bootstrap=False, n_jobs = 4)\n",
    "\n",
    "# create a pipeline that transforms data to semantic space, \n",
    "# and then classifies them using clf\n",
    "estimators = [('projection', bow2Top), ('clf', clf)]\n",
    "semClf = Pipeline(estimators)\n",
    "\n",
    "grid_DT = GridSearchCV(estimator = semClf, param_grid = params, scoring = ['f1', 'accuracy', 'precision', 'recall'], \n",
    "                       refit = 'f1', cv = cv, verbose = 1, n_jobs = 4)\n",
    "\n",
    "grid_DT.fit(bow_train, train_lbls)\n",
    "\n",
    "# cross validate over the pipeline using group k-fold CV\n",
    "#pred_lbls = cross_val_predict(semClf, bow_train, train_lbls, cv=cv)\n",
    "\n",
    "time1 = time.time()\n",
    "print(time1-time0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([242.39180713, 233.35279417, 242.97810702, 229.0049798 ,\n",
      "       218.64869013]), 'std_fit_time': array([ 6.59821453,  4.88109719,  8.38355339, 11.24679564, 66.04149978]), 'mean_score_time': array([5.90632501, 4.93922744, 4.61299577, 5.00871372, 4.72566414]), 'std_score_time': array([2.13824477, 1.22147921, 1.09948707, 1.51876656, 1.91014334]), 'param_clf__base_estimator__max_depth': masked_array(data=[3, 5, 7, 10, 15],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__base_estimator__max_leaf_nodes': masked_array(data=[20, 20, 20, 20, 20],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__base_estimator__min_samples_leaf': masked_array(data=[1, 1, 1, 1, 1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__max_samples': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_clf__n_estimators': masked_array(data=[10, 10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'clf__base_estimator__max_depth': 3, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}, {'clf__base_estimator__max_depth': 5, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}, {'clf__base_estimator__max_depth': 7, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}, {'clf__base_estimator__max_depth': 10, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}, {'clf__base_estimator__max_depth': 15, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}], 'split0_test_f1': array([0.8120598 , 0.81175813, 0.8128242 , 0.80873076, 0.81361851]), 'split1_test_f1': array([0.80154238, 0.80302165, 0.80311471, 0.80391661, 0.80131239]), 'split2_test_f1': array([0.78185365, 0.78339701, 0.78343949, 0.7831099 , 0.78318412]), 'split3_test_f1': array([0.80317187, 0.80300448, 0.80130334, 0.80103424, 0.80240722]), 'split4_test_f1': array([0.81242387, 0.80964782, 0.81058148, 0.81239179, 0.80967863]), 'mean_test_f1': array([0.80221032, 0.80216582, 0.80225264, 0.80183666, 0.80204017]), 'std_test_f1': array([0.0111094 , 0.01001695, 0.01036056, 0.01014496, 0.01047491]), 'rank_test_f1': array([2, 3, 1, 5, 4]), 'split0_test_accuracy': array([0.69570871, 0.6970091 , 0.70041012, 0.69670901, 0.7010103 ]), 'split1_test_accuracy': array([0.68077231, 0.68957583, 0.68887555, 0.68947579, 0.68497399]), 'split2_test_accuracy': array([0.65312656, 0.66533267, 0.66663332, 0.66493247, 0.66563282]), 'split3_test_accuracy': array([0.67963982, 0.68774387, 0.68274137, 0.68434217, 0.68464232]), 'split4_test_accuracy': array([0.69187675, 0.69287715, 0.69627851, 0.69647859, 0.69547819]), 'mean_test_accuracy': array([0.68022483, 0.68650772, 0.68698777, 0.68638761, 0.68634752]), 'std_test_accuracy': array([0.01490286, 0.01104693, 0.01185223, 0.01168249, 0.01210173]), 'rank_test_accuracy': array([5, 2, 1, 3, 4]), 'split0_test_precision': array([0.70078908, 0.70354411, 0.70738605, 0.70875719, 0.70690216]), 'split1_test_precision': array([0.68393122, 0.6953606 , 0.69405843, 0.69366619, 0.6908181 ]), 'split2_test_precision': array([0.65634904, 0.67091837, 0.67273133, 0.67073441, 0.67163681]), 'split3_test_precision': array([0.68115942, 0.69397774, 0.68886016, 0.69183007, 0.69002695]), 'split4_test_precision': array([0.69464695, 0.70128894, 0.7053203 , 0.70226641, 0.70564516]), 'mean_test_precision': array([0.68337514, 0.69301795, 0.69367126, 0.69345085, 0.69300584]), 'std_test_precision': array([0.01527041, 0.01161043, 0.01253454, 0.01288959, 0.01283087]), 'rank_test_precision': array([5, 3, 1, 2, 4]), 'split0_test_recall': array([0.9653349 , 0.95931257, 0.95519976, 0.94153937, 0.95828437]), 'split1_test_recall': array([0.96800361, 0.95012769, 0.95283161, 0.95583596, 0.95388313]), 'split2_test_recall': array([0.96670297, 0.94118562, 0.93776256, 0.94071884, 0.93916291]), 'split3_test_recall': array([0.97843343, 0.95267336, 0.9576157 , 0.95117568, 0.9585143 ]), 'split4_test_recall': array([0.97829275, 0.9576122 , 0.95277207, 0.96347903, 0.94969199]), 'mean_test_recall': array([0.97135353, 0.95218229, 0.95123634, 0.95054977, 0.95190734]), 'std_test_recall': array([0.00578535, 0.00641427, 0.00696923, 0.00864102, 0.00714913]), 'rank_test_recall': array([1, 2, 4, 5, 3])}\n",
      "Pipeline(memory=None,\n",
      "         steps=[('projection',\n",
      "                 docTopTransformer(d=300, distributed=False,\n",
      "                                   this_dict=<gensim.corpora.dictionary.Dictionary object at 0x000001A976CE6248>)),\n",
      "                ('clf',\n",
      "                 BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                                         class_weight=None,\n",
      "                                                                         criterion='gini',\n",
      "                                                                         max_depth=7,\n",
      "                                                                         max_features=None,\n",
      "                                                                         max_leaf_nodes=20,\n",
      "                                                                         min_impurity_decrease=0.0,\n",
      "                                                                         min_impurity_split=None,\n",
      "                                                                         min_samples_leaf=1,\n",
      "                                                                         min_samples_split=2,\n",
      "                                                                         min_weight_fraction_leaf=0.0,\n",
      "                                                                         presort='deprecated',\n",
      "                                                                         random_state=None,\n",
      "                                                                         splitter='best'),\n",
      "                                   bootstrap=False, bootstrap_features=False,\n",
      "                                   max_features=1.0, max_samples=0.1,\n",
      "                                   n_estimators=10, n_jobs=4, oob_score=False,\n",
      "                                   random_state=None, verbose=0,\n",
      "                                   warm_start=False))],\n",
      "         verbose=False)\n",
      "best score:\n",
      "0.8022526447465935\n",
      "best params:\n",
      "{'clf__base_estimator__max_depth': 7, 'clf__base_estimator__max_leaf_nodes': 20, 'clf__base_estimator__min_samples_leaf': 1, 'clf__max_samples': 0.1, 'clf__n_estimators': 10}\n",
      "best index:\n",
      "2\n",
      "scorer:\n",
      "{'f1': make_scorer(f1_score, average=binary), 'accuracy': make_scorer(accuracy_score), 'precision': make_scorer(precision_score, average=binary), 'recall': make_scorer(recall_score, average=binary)}\n",
      "refit time:\n",
      "93.54198670387268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(grid_DT.cv_results_)\n",
    "print(grid_DT.best_estimator_)\n",
    "print(\"best score:\")\n",
    "print(grid_DT.best_score_)\n",
    "print(\"best params:\")\n",
    "print(grid_DT.best_params_)\n",
    "print(\"best index:\")\n",
    "print(grid_DT.best_index_)\n",
    "print(\"scorer:\")\n",
    "print(grid_DT.scorer_)\n",
    "print(\"refit time:\")\n",
    "print(grid_DT.refit_time_)\n",
    "\n",
    "\n",
    "#report = classification_report(train_lbls, pred_lbls)\n",
    "#print('True recall is sensitivity, false recall is specificity')\n",
    "#print(report)\n",
    "\n",
    "# this is not exactly the same as the average of each CV folds accuracy, \n",
    "# but it's close and much faster to compute\n",
    "#acc = 1-np.mean(pred_lbls != train_lbls)\n",
    "#print(\"Accuracy: %0.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680.05286502838\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
